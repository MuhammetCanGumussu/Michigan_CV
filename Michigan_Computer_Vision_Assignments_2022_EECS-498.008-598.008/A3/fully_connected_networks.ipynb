{"cells":[{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"FrfeHl_-m4V-","new_sheet":false,"run_control":{"read_only":false}},"source":["## Setup Code\n","Before getting started, we need to run some boilerplate code to set up our environment, same as Assignment 1. You'll need to rerun this setup code each time you start the notebook.\n","\n","First, run this cell load the autoreload extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."]},{"cell_type":"code","execution_count":1,"metadata":{"button":false,"deletable":true,"executionInfo":{"elapsed":616,"status":"ok","timestamp":1677960505472,"user":{"displayName":"Muhammet Can Gümüşsu","userId":"11484611351929850578"},"user_tz":-180},"id":"VyQblYp0nEZq","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"Q7ymI0aZ2W1b","new_sheet":false,"run_control":{"read_only":false}},"source":["### Google Colab Setup\n","Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n","\n","Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."]},{"cell_type":"code","execution_count":2,"metadata":{"button":false,"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"executionInfo":{"elapsed":21736,"status":"ok","timestamp":1677960527910,"user":{"displayName":"Muhammet Can Gümüşsu","userId":"11484611351929850578"},"user_tz":-180},"id":"c_LLpLyC2eau","new_sheet":false,"outputId":"0ddedbb6-2772-4b0e-bfbf-5ff36a40841a","run_control":{"read_only":false}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"mbq-UT8J2mnv","new_sheet":false,"run_control":{"read_only":false}},"source":["Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the folowing cell should print the filenames from the assignment:\n","\n","```\n","['convolutional_networks.ipynb', 'fully_connected_networks.ipynb', 'eecs598', 'convolutional_networks.py', 'fully_connected_networks.py', 'a3_helper.py']\n","```"]},{"cell_type":"code","execution_count":3,"metadata":{"button":false,"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"executionInfo":{"elapsed":1083,"status":"ok","timestamp":1677960528988,"user":{"displayName":"Muhammet Can Gümüşsu","userId":"11484611351929850578"},"user_tz":-180},"id":"WcrhTOZW243H","new_sheet":false,"outputId":"91b6aa97-29c1-457b-ed16-90b8b341e4b3","run_control":{"read_only":false}},"outputs":[{"output_type":"stream","name":"stdout","text":["['convolutional_networks.ipynb', 'convolutional_networks.py', 'a3_helper.py', 'eecs598', '__pycache__', 'best_two_layer_net.pth', 'fully_connected_networks.py', 'fully_connected_networks.ipynb']\n"]}],"source":["import os\n","\n","# TODO: Fill in the Google Drive path where you uploaded the assignment\n","# Example: If you create a 2022WI folder and put all the files under A3 folder, then '2022WI/A3'\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = \"/content/drive/MyDrive/Mich.ComputerVisionAssignment1/A3\"\n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"xegb0uDA232J","new_sheet":false,"run_control":{"read_only":false}},"source":["Once you have successfully mounted your Google Drive and located the path to this assignment, run th following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n","\n","```\n","Hello from fully_connected_networks.py!\n","Hello from a3_helper.py!\n","```\n","\n","as well as the last edit time for the file `fully_connected_networks.py`."]},{"cell_type":"code","execution_count":4,"metadata":{"button":false,"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"executionInfo":{"elapsed":9375,"status":"ok","timestamp":1677960538361,"user":{"displayName":"Muhammet Can Gümüşsu","userId":"11484611351929850578"},"user_tz":-180},"id":"AhGQF5sw3Fas","new_sheet":false,"outputId":"616e79ce-07b3-460e-88d6-cd82c08b1baa","run_control":{"read_only":false}},"outputs":[{"output_type":"stream","name":"stdout","text":["Hello from fully_connected_networks.py!\n","Hello from a3_helper.py!\n","fully_connected_networks.py last edited on Sat Mar  4 15:08:04 2023\n"]}],"source":["import sys\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","\n","import time, os\n","os.environ[\"TZ\"] = \"US/Eastern\"\n","time.tzset()\n","\n","from fully_connected_networks import hello_fully_connected_networks\n","hello_fully_connected_networks()\n","\n","from a3_helper import hello_helper\n","hello_helper()\n","\n","fully_connected_networks_path = os.path.join(GOOGLE_DRIVE_PATH, 'fully_connected_networks.py')\n","fully_connected_networks_edit_time = time.ctime(os.path.getmtime(fully_connected_networks_path))\n","print('fully_connected_networks.py last edited on %s' % fully_connected_networks_edit_time)"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"ynKS05gJ4iBo","new_sheet":false,"run_control":{"read_only":false}},"source":["# Data preprocessing"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"fN1SShPR4lJV","new_sheet":false,"run_control":{"read_only":false}},"source":["## Setup code\n","Run some setup code for this notebook: Import some useful packages and increase the default figure size."]},{"cell_type":"code","execution_count":5,"metadata":{"button":false,"deletable":true,"id":"VUCKw4Tl1ddj","new_sheet":false,"run_control":{"read_only":false},"executionInfo":{"status":"ok","timestamp":1677960538362,"user_tz":-180,"elapsed":7,"user":{"displayName":"Muhammet Can Gümüşsu","userId":"11484611351929850578"}}},"outputs":[],"source":["import eecs598\n","import torch\n","import torchvision\n","import matplotlib.pyplot as plt\n","import statistics\n","import random\n","import time\n","import math\n","%matplotlib inline\n","\n","from eecs598 import reset_seed, Solver\n","\n","plt.rcParams['figure.figsize'] = (10.0, 8.0)\n","plt.rcParams['font.size'] = 16"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"lhqpd2IN2O-K","new_sheet":false,"run_control":{"read_only":false}},"source":["Starting in this assignment, we will use the GPU to accelerate our computation. Run this cell to make sure you are using a GPU."]},{"cell_type":"code","execution_count":6,"metadata":{"button":false,"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"executionInfo":{"elapsed":7,"status":"ok","timestamp":1677960538363,"user":{"displayName":"Muhammet Can Gümüşsu","userId":"11484611351929850578"},"user_tz":-180},"id":"SGDxdBIMRX6b","new_sheet":false,"outputId":"76c36df2-5fd2-4098-ea4b-768e21755758","run_control":{"read_only":false}},"outputs":[{"output_type":"stream","name":"stdout","text":["Good to go!\n"]}],"source":["if torch.cuda.is_available:\n","  print('Good to go!')\n","else:\n","  print('Please set GPU via Edit -> Notebook Settings.')"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"-Yv3zQYw5B3s","new_sheet":false,"run_control":{"read_only":false}},"source":["## Load the CIFAR-10 dataset\n","Then, we will first load the CIFAR-10 dataset. The utility function `eecs598.data.preprocess_cifar10()` returns the entire CIFAR-10 dataset as a set of six **Torch tensors** while also preprocessing the RGB images:\n","\n","- `X_train` contains all training images (real numbers in the range $[0, 1]$)\n","- `y_train` contains all training labels (integers in the range $[0, 9]$)\n","- `X_val` contains all validation images\n","- `y_val` contains all validation labels\n","- `X_test` contains all test images\n","- `y_test` contains all test labels"]},{"cell_type":"code","execution_count":7,"metadata":{"button":false,"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["39dfa3c6eb594d50b5caae902427e54e","c849dbdfdd94403cb12436580b03a77e","0b06e1193c094791bb4da0d537ff4aee","0748226e6f0e47438585d8db3d66490e","c75907f5539f4740b38ffe5105af2bd4","819fdc5a53b6486e9bb5f539faaca829","703f2de518124ba99b19f44299fa5664","7764548275034dfbaf3092ff092ce15d","bd62f274ddf34fb8a95b42b10ec62394","fc9b50a9957141538b417210eb32ff54","edc0d90cf2e3406390ec7122d15b08cf"]},"deletable":true,"executionInfo":{"elapsed":12873,"status":"error","timestamp":1677960551230,"user":{"displayName":"Muhammet Can Gümüşsu","userId":"11484611351929850578"},"user_tz":-180},"id":"V2mFlFmQ1ddm","new_sheet":false,"outputId":"18cb3e7c-4db1-46f3-bcc2-6f0adc732788","run_control":{"read_only":false}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39dfa3c6eb594d50b5caae902427e54e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./cifar-10-python.tar.gz to .\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-20590442d6af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0meecs598\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meecs598\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_cifar10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train data shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train labels shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Mich.ComputerVisionAssignment1/A3/eecs598/data.py\u001b[0m in \u001b[0;36mpreprocess_cifar10\u001b[0;34m(cuda, show_examples, bias_trick, flatten, validation_ratio, dtype)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m# Move data to the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LAZY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"]}],"source":["# Invoke the above function to get our data. \n","import eecs598\n","\n","eecs598.reset_seed(0)\n","data_dict = eecs598.data.preprocess_cifar10(cuda=True, dtype=torch.float64)\n","print('Train data shape: ', data_dict['X_train'].shape)\n","print('Train labels shape: ', data_dict['y_train'].shape)\n","print('Validation data shape: ', data_dict['X_val'].shape)\n","print('Validation labels shape: ', data_dict['y_val'].shape)\n","print('Test data shape: ', data_dict['X_test'].shape)\n","print('Test labels shape: ', data_dict['y_test'].shape)"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"ZeH0OvuEe1CN","new_sheet":false,"run_control":{"read_only":false},"tags":["pdf-title"]},"source":["# Fully-connected neural networks\n","In the previous homework you implemented a fully-connected two-layer neural network on CIFAR-10. The implementation was simple but not very modular since the loss and gradient were computed in a single monolithic function. This is manageable for a simple two-layer network, but would become impractical as we move to bigger models. Ideally we want to build networks using a more modular design so that we can implement different layer types in isolation and then snap them together into models with different architectures."]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"3Qiu9_4pe1CP","new_sheet":false,"run_control":{"read_only":false},"tags":["pdf-ignore"]},"source":["In this exercise we will implement fully-connected networks using a more modular approach. For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n","\n","```python\n","def forward(x, w):\n","  \"\"\" Receive inputs x and weights w \"\"\"\n","  # Do some computations ...\n","  z = # ... some intermediate value\n","  # Do some more computations ...\n","  out = # the output\n","   \n","  cache = (x, w, z, out) # Values we need to compute gradients\n","   \n","  return out, cache\n","```\n","\n","The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n","\n","```python\n","def backward(dout, cache):\n","  \"\"\"\n","  Receive dout (derivative of loss with respect to outputs) and cache,\n","  and compute derivative with respect to inputs.\n","  \"\"\"\n","  # Unpack cache values\n","  x, w, z, out = cache\n","  \n","  # Use values in cache to compute derivatives\n","  dx = # Derivative of loss with respect to x\n","  dw = # Derivative of loss with respect to w\n","  \n","  return dx, dw\n","```\n","\n","After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures.\n","\n","In addition to implementing fully-connected networks of arbitrary depth, we will also explore different update rules for optimization, and introduce Dropout as a regularizer as a tool to more efficiently optimize deep networks.\n","\n","To validate our implementation, we will compare the analytically computed gradients with numerical approximations of the gradient as done in previous assignments. You can inspect the numeric gradient function `compute_numeric_gradient` in `eecs598/grad.py`. Please note that we have updated the function to accept upstream gradients to allow us to debug intermediate layers easily. You can check the update description by running the code block below.\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"executionInfo":{"elapsed":302,"status":"ok","timestamp":1677951372611,"user":{"displayName":"Muhammet Can Gümüşsu","userId":"11484611351929850578"},"user_tz":-180},"id":"DfseUp7H-TF_","new_sheet":false,"outputId":"0988ac90-248b-4875-b783-acaa96d9a93b","run_control":{"read_only":false}},"outputs":[{"output_type":"stream","name":"stdout","text":["Help on function compute_numeric_gradient in module eecs598.grad:\n","\n","compute_numeric_gradient(f, x, dLdf=None, h=1e-07)\n","    Compute the numeric gradient of f at x using a finite differences\n","    approximation. We use the centered difference:\n","    \n","    df    f(x + h) - f(x - h)\n","    -- ~= -------------------\n","    dx           2 * h\n","    \n","    Function can also expand this easily to intermediate layers using the\n","    chain rule:\n","    \n","    dL   df   dL\n","    -- = -- * --\n","    dx   dx   df\n","    \n","    Inputs:\n","    - f: A function that inputs a torch tensor and returns a torch scalar\n","    - x: A torch tensor giving the point at which to compute the gradient\n","    - dLdf: optional upstream gradient for intermediate layers\n","    - h: epsilon used in the finite difference calculation\n","    Returns:\n","    - grad: A tensor of the same shape as x giving the gradient of f at x\n","\n"]}],"source":["help(eecs598.grad.compute_numeric_gradient)"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"JB7Eu3qJ9xnm","new_sheet":false,"run_control":{"read_only":false}},"source":["# Linear layer"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"bRdnxsvZunFu","new_sheet":false,"run_control":{"read_only":false}},"source":["For each layer we implement, we will define a class with two static methods `forward` and `backward`. The class structure is currently provided in `fully_connected_layers.py`, you will be implementing both the `forward` and `backward` methods."]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"0NNv3l-ne1Cb","new_sheet":false,"run_control":{"read_only":false}},"source":["## Linear layer: forward\n","Implement the `Linear.forward` function in `fully_connected_layers.py`. Once you are done you can test your implementaion by running the next cell. You should see errors less than `1e-7`."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"executionInfo":{"elapsed":3207,"status":"ok","timestamp":1677954403469,"user":{"displayName":"Muhammet Can Gümüşsu","userId":"11484611351929850578"},"user_tz":-180},"id":"sjq2Sq4Ze1Cc","new_sheet":false,"outputId":"5604bd7c-c578-4e12-a7d5-a0cf1893e60d","run_control":{"read_only":false}},"outputs":[{"output_type":"stream","name":"stdout","text":["Testing Linear.forward function:\n","difference:  3.683042917976506e-08\n"]}],"source":["from fully_connected_networks import Linear\n","\n","# Test the Linear.forward function\n","num_inputs = 2\n","input_shape = torch.tensor((4, 5, 6))\n","output_dim = 3\n","\n","input_size = num_inputs * torch.prod(input_shape)\n","weight_size = output_dim * torch.prod(input_shape)\n","\n","x = torch.linspace(-0.1, 0.5, steps=input_size, dtype=torch.float64, device='cuda')\n","w = torch.linspace(-0.2, 0.3, steps=weight_size, dtype=torch.float64, device='cuda')\n","b = torch.linspace(-0.3, 0.1, steps=output_dim, dtype=torch.float64, device='cuda')\n","x = x.reshape(num_inputs, *input_shape)\n","w = w.reshape(torch.prod(input_shape), output_dim)\n","\n","out, _ = Linear.forward(x, w, b)\n","correct_out = torch.tensor([[1.49834984, 1.70660150, 1.91485316],\n","                            [3.25553226, 3.51413301, 3.77273372]]\n","                            ).double().cuda()\n","\n","print('Testing Linear.forward function:')\n","print('difference: ', eecs598.grad.rel_error(out, correct_out))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"4mxIDo46e1Cf","new_sheet":false,"run_control":{"read_only":false}},"source":["## Linear layer: backward\n","Now implement the `Linear.backward` function and test your implementation using numeric gradient checking. \n","\n","Run the following to test your implementation of `Linear.backward`. You should see errors less than `1e-7`."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"executionInfo":{"elapsed":489,"status":"ok","timestamp":1677954407359,"user":{"displayName":"Muhammet Can Gümüşsu","userId":"11484611351929850578"},"user_tz":-180},"id":"ts85gmote1Cg","new_sheet":false,"outputId":"e08f7047-7e40-42eb-a747-c32f646dbedd","run_control":{"read_only":false}},"outputs":[{"output_type":"stream","name":"stdout","text":["Testing Linear.backward function:\n","dx error:  5.221943563709987e-10\n","dw error:  3.498388787266994e-10\n","db error:  5.373171200544344e-10\n"]}],"source":["from fully_connected_networks import Linear\n","\n","# Test the Linear.backward function\n","reset_seed(0)\n","x = torch.randn(10, 2, 3, dtype=torch.float64, device='cuda')\n","w = torch.randn(6, 5, dtype=torch.float64, device='cuda')\n","b = torch.randn(5, dtype=torch.float64, device='cuda')\n","dout = torch.randn(10, 5, dtype=torch.float64, device='cuda')\n","\n","dx_num = eecs598.grad.compute_numeric_gradient(lambda x: Linear.forward(x, w, b)[0], x, dout)\n","dw_num = eecs598.grad.compute_numeric_gradient(lambda w: Linear.forward(x, w, b)[0], w, dout)\n","db_num = eecs598.grad.compute_numeric_gradient(lambda b: Linear.forward(x, w, b)[0], b, dout)\n","\n","_, cache = Linear.forward(x, w, b)\n","dx, dw, db = Linear.backward(dout, cache)\n","\n","# The error should be around e-10 or less\n","print('Testing Linear.backward function:')\n","print('dx error: ', eecs598.grad.rel_error(dx_num, dx))\n","print('dw error: ', eecs598.grad.rel_error(dw_num, dw))\n","print('db error: ', eecs598.grad.rel_error(db_num, db))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"bdIqQzqiJQE6","new_sheet":false,"run_control":{"read_only":false}},"source":["# ReLU activation"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"YdX98A_qvTRt","new_sheet":false,"run_control":{"read_only":false}},"source":["We will now implement the ReLU nonlinearity. As above, we will define a class with two empty static methods, and implement them in upcoming cells. The class structure can be found in `fully_connected_networks.py`"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"n2DyqL4Ae1Cl","new_sheet":false,"run_control":{"read_only":false}},"source":["## ReLU activation: forward\n","Implement the forward pass for the ReLU activation function in the `ReLU.forward` function. You **should not** change the input tensor with an in-place operation. \n","\n","Run the following to test your implementation of the ReLU forward pass. Your errors should be less than `1e-7`."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"QblpieUJe1Cm","new_sheet":false,"run_control":{"read_only":false},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677954411098,"user_tz":-180,"elapsed":4,"user":{"displayName":"Muhammet Can Gümüşsu","userId":"11484611351929850578"}},"outputId":"e1d3c3d2-af55-45d3-d2d6-8bf76e39e0c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Testing ReLU.forward function:\n","difference:  4.5454545613554664e-09\n"]}],"source":["from fully_connected_networks import ReLU\n","\n","reset_seed(0)\n","x = torch.linspace(-0.5, 0.5, steps=12, dtype=torch.float64, device='cuda')\n","x = x.reshape(3, 4)\n","\n","out, _ = ReLU.forward(x)\n","correct_out = torch.tensor([[ 0.,          0.,          0.,          0.,        ],\n","                            [ 0.,          0.,          0.04545455,  0.13636364,],\n","                            [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]],\n","                            dtype=torch.float64,\n","                            device='cuda')\n","\n","# Compare your output with ours. The error should be on the order of e-8\n","print('Testing ReLU.forward function:')\n","print('difference: ', eecs598.grad.rel_error(out, correct_out))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"3bSInb7xe1Cq","new_sheet":false,"run_control":{"read_only":false}},"source":["## ReLU activation: backward\n","Now implement the backward pass for the ReLU activation function.\n","\n","Again, you should not change the input tensor with an in-place operation.\n","\n","Run the following to test your implementation of `ReLU.backward`. Your errors should be less than `1e-8`."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"odiV48zBe1Cr","new_sheet":false,"run_control":{"read_only":false},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677954416573,"user_tz":-180,"elapsed":410,"user":{"displayName":"Muhammet Can Gümüşsu","userId":"11484611351929850578"}},"outputId":"079a3d16-711d-4a4a-fcfa-4d46cb14f5c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Testing ReLU.backward function:\n","dx error:  2.6317796097761553e-10\n"]}],"source":["from fully_connected_networks import ReLU\n","\n","reset_seed(0)\n","x = torch.randn(10, 10, dtype=torch.float64, device='cuda')\n","dout = torch.randn(*x.shape, dtype=torch.float64, device='cuda')\n","\n","dx_num = eecs598.grad.compute_numeric_gradient(lambda x: ReLU.forward(x)[0], x, dout)\n","\n","_, cache = ReLU.forward(x)\n","dx = ReLU.backward(dout, cache)\n","\n","# The error should be on the order of e-12\n","print('Testing ReLU.backward function:')\n","print('dx error: ', eecs598.grad.rel_error(dx_num, dx))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"eVTMuUOZe1Cv","new_sheet":false,"run_control":{"read_only":false}},"source":["# \"Sandwich\" layers\n","There are some common patterns of layers that are frequently used in neural nets. For example, linear layers are frequently followed by a ReLU nonlinearity. To make these common patterns easy, we define a convenience layer.\n","\n","This also shows how our layer abstraction allows us to implement new layers by composing existing layer implementations. This is a powerful mechanism for structuring deep learning code in a modular fashion.\n","\n","For now take a look at the `forward` and `backward` functions in `Linear_ReLU`, and run the following to numerically gradient check the backward pass.\n","\n","Run the following to test the implementation of the `Linear_ReLU` layer using numeric gradient checking. You should see errors less than `1e-8`"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"-gaY5YfAe1Cw","new_sheet":false,"run_control":{"read_only":false},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677954421284,"user_tz":-180,"elapsed":411,"user":{"displayName":"Muhammet Can Gümüşsu","userId":"11484611351929850578"}},"outputId":"5d56314a-6b6d-4962-b849-f5f1b5b1d470"},"outputs":[{"output_type":"stream","name":"stdout","text":["Testing Linear_ReLU.forward and Linear_ReLU.backward:\n","dx error:  1.210759699545244e-09\n","dw error:  7.462948482161807e-10\n","db error:  8.915028842081707e-10\n"]}],"source":["from fully_connected_networks import Linear_ReLU\n","\n","reset_seed(0)\n","x = torch.randn(2, 3, 4, dtype=torch.float64, device='cuda')\n","w = torch.randn(12, 10, dtype=torch.float64, device='cuda')\n","b = torch.randn(10, dtype=torch.float64, device='cuda')\n","dout = torch.randn(2, 10, dtype=torch.float64, device='cuda')\n","\n","out, cache = Linear_ReLU.forward(x, w, b)\n","dx, dw, db = Linear_ReLU.backward(dout, cache)\n","\n","dx_num = eecs598.grad.compute_numeric_gradient(lambda x: Linear_ReLU.forward(x, w, b)[0], x, dout)\n","dw_num = eecs598.grad.compute_numeric_gradient(lambda w: Linear_ReLU.forward(x, w, b)[0], w, dout)\n","db_num = eecs598.grad.compute_numeric_gradient(lambda b: Linear_ReLU.forward(x, w, b)[0], b, dout)\n","\n","# Relative error should be around e-8 or less\n","print('Testing Linear_ReLU.forward and Linear_ReLU.backward:')\n","print('dx error: ', eecs598.grad.rel_error(dx_num, dx))\n","print('dw error: ', eecs598.grad.rel_error(dw_num, dw))\n","print('db error: ', eecs598.grad.rel_error(db_num, db))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"rAGgiyP5e1C0","new_sheet":false,"run_control":{"read_only":false}},"source":["# Loss layers: Softmax and SVM\n","You implemented these loss functions in the last assignment, so we'll give them to you for free in `helper_functions.py`. You should still make sure you understand how they work by looking at the implementations. We can first verify our implementations.\n","\n","\n","Run the following to perform numeric gradient checking on the two loss functions. You should see errors less than `1e-7` for svm_loss and `1e-6` for softmax_loss."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"BU9xp64De1C1","new_sheet":false,"run_control":{"read_only":false},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677954427223,"user_tz":-180,"elapsed":927,"user":{"displayName":"Muhammet Can Gümüşsu","userId":"11484611351929850578"}},"outputId":"c5af2aa2-1753-4717-d8b0-dc648f6c918e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Testing svm_loss:\n","loss:  9.000430792478463\n","dx error:  7.97306008441663e-09\n","\n","Testing softmax_loss:\n","loss:  2.3026286102347924\n","dx error:  1.0417990899757076e-07\n"]}],"source":["from a3_helper import svm_loss, softmax_loss\n","\n","reset_seed(0)\n","num_classes, num_inputs = 10, 50\n","x = 0.001 * torch.randn(num_inputs, num_classes, dtype=torch.float64, device='cuda')\n","y = torch.randint(num_classes, size=(num_inputs,), dtype=torch.int64, device='cuda')\n","\n","dx_num = eecs598.grad.compute_numeric_gradient(lambda x: svm_loss(x, y)[0], x)\n","loss, dx = svm_loss(x, y)\n","\n","# Test svm_loss function. Loss should be around 9 and dx error should be around the order of e-9\n","print('Testing svm_loss:')\n","print('loss: ', loss.item())\n","print('dx error: ', eecs598.grad.rel_error(dx_num, dx))\n","\n","dx_num = eecs598.grad.compute_numeric_gradient(lambda x: softmax_loss(x, y)[0], x)\n","loss, dx = softmax_loss(x, y)\n","\n","# Test softmax_loss function. Loss should be close to 2.3 and dx error should be around e-8\n","print('\\nTesting softmax_loss:')\n","print('loss: ', loss.item())\n","print('dx error: ', eecs598.grad.rel_error(dx_num, dx))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"qq7-cyfQe1C4","new_sheet":false,"run_control":{"read_only":false}},"source":["# Two-layer network\n","In the previous assignment you implemented a two-layer neural network in a single monolithic class. Now that you have implemented modular versions of the necessary layers, you will reimplement the two layer network using these modular implementations.\n","\n","Complete the implementation of the `TwoLayerNet` class. This class will serve as a model for the other networks you will implement in this assignment, so read through it to make sure you understand the API. \n","\n","Once you have finished implementing the forward and backward passes of your two-layer net, run the following to test your implementation:"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"d3JOcfyze1C5","new_sheet":false,"run_control":{"read_only":false},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677954435418,"user_tz":-180,"elapsed":3485,"user":{"displayName":"Muhammet Can Gümüşsu","userId":"11484611351929850578"}},"outputId":"95ba2c2a-3b7f-49fb-93ef-d18d83e29e37"},"outputs":[{"output_type":"stream","name":"stdout","text":["Testing initialization ... \n","Testing test-time forward pass ... \n","Testing training loss (no regularization)\n","Running numeric gradient check with reg =  0.0\n","W1 relative error: 2.94e-07\n","W2 relative error: 1.65e-09\n","b1 relative error: 1.01e-06\n","b2 relative error: 4.63e-09\n","Running numeric gradient check with reg =  0.7\n","W1 relative error: 2.70e-08\n","W2 relative error: 9.86e-09\n","b1 relative error: 2.28e-06\n","b2 relative error: 2.90e-08\n"]}],"source":["from fully_connected_networks import TwoLayerNet\n","from a3_helper import svm_loss, softmax_loss\n","\n","reset_seed(0)\n","N, D, H, C = 3, 5, 50, 7\n","X = torch.randn(N, D, dtype=torch.float64, device='cuda')\n","y = torch.randint(C, size=(N,), dtype=torch.int64, device='cuda')\n","\n","std = 1e-3\n","model = TwoLayerNet(\n","          input_dim=D,\n","          hidden_dim=H,\n","          num_classes=C,\n","          weight_scale=std,\n","          dtype=torch.float64,\n","          device='cpu'\n","        )\n","\n","print('Testing initialization ... ')\n","W1_std = torch.abs(model.params['W1'].std() - std)\n","b1 = model.params['b1']\n","W2_std = torch.abs(model.params['W2'].std() - std)\n","b2 = model.params['b2']\n","assert W1_std < std / 10, 'First layer weights do not seem right'\n","assert torch.all(b1 == 0), 'First layer biases do not seem right'\n","assert W2_std < std / 10, 'Second layer weights do not seem right'\n","assert torch.all(b2 == 0), 'Second layer biases do not seem right'\n","\n","print('Testing test-time forward pass ... ')\n","model.params['W1'] = torch.linspace(-0.7, 0.3, steps=D * H, dtype=torch.float64, device='cuda').reshape(D, H)\n","model.params['b1'] = torch.linspace(-0.1, 0.9, steps=H, dtype=torch.float64, device='cuda')\n","model.params['W2'] = torch.linspace(-0.3, 0.4, steps=H * C, dtype=torch.float64, device='cuda').reshape(H, C)\n","model.params['b2'] = torch.linspace(-0.9, 0.1, steps=C, dtype=torch.float64, device='cuda')\n","X = torch.linspace(-5.5, 4.5, steps=N * D, dtype=torch.float64, device='cuda').reshape(D, N).t()\n","scores = model.loss(X)\n","correct_scores = torch.tensor(\n","  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n","   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n","   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]],\n","    dtype=torch.float64, device='cuda')\n","scores_diff = torch.abs(scores - correct_scores).sum()\n","assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n","\n","print('Testing training loss (no regularization)')\n","y = torch.tensor([0, 5, 1])\n","loss, grads = model.loss(X, y)\n","correct_loss = 3.4702243556\n","assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n","\n","model.reg = 1.0\n","loss, grads = model.loss(X, y)\n","correct_loss = 49.719461034881775\n","assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n","\n","# Errors should be around e-6 or less\n","for reg in [0.0, 0.7]:\n","  print('Running numeric gradient check with reg = ', reg)\n","  model.reg = reg\n","  loss, grads = model.loss(X, y)\n","\n","  for name in sorted(grads):\n","    f = lambda _: model.loss(X, y)[0]\n","    grad_num = eecs598.grad.compute_numeric_gradient(f, model.params[name])\n","    print('%s relative error: %.2e' % (name, eecs598.grad.rel_error(grad_num, grads[name])))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"q1Odj9XQe1C9","new_sheet":false,"run_control":{"read_only":false}},"source":["# Solver\n","In the previous assignment, the logic for training models was coupled to the models themselves. Following a more modular design, for this assignment we have split the logic for training models into a separate class.\n","\n","Read through `help(Solver)` to familiarize yourself with the API. After doing so, use a `Solver` instance to train a `TwoLayerNet` that achieves at least `50%` accuracy on the validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"lZ-8wKffRoDu","new_sheet":false,"run_control":{"read_only":false},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675361908315,"user_tz":-180,"elapsed":9,"user":{"displayName":"Muhammet Can Gümüşsu","userId":"11484611351929850578"}},"outputId":"983f8c3a-47da-47b3-8b0a-9492ae8a8c01"},"outputs":[{"output_type":"stream","name":"stdout","text":["Help on class Solver in module eecs598.solver:\n","\n","class Solver(builtins.object)\n"," |  Solver(model, data, **kwargs)\n"," |  \n"," |  A Solver encapsulates all the logic necessary for training classification\n"," |  models. The Solver performs stochastic gradient descent using different\n"," |  update rules.\n"," |  The solver accepts both training and validation data and labels so it can\n"," |  periodically check classification accuracy on both training and validation\n"," |  data to watch out for overfitting.\n"," |  To train a model, you will first construct a Solver instance, passing the\n"," |  model, dataset, and various options (learning rate, batch size, etc) to the\n"," |  constructor. You will then call the train() method to run the optimization\n"," |  procedure and train the model.\n"," |  After the train() method returns, model.params will contain the parameters\n"," |  that performed best on the validation set over the course of training.\n"," |  In addition, the instance variable solver.loss_history will contain a list\n"," |  of all losses encountered during training and the instance variables\n"," |  solver.train_acc_history and solver.val_acc_history will be lists of the\n"," |  accuracies of the model on the training and validation set at each epoch.\n"," |  Example usage might look something like this:\n"," |  data = {\n"," |    'X_train': # training data\n"," |    'y_train': # training labels\n"," |    'X_val': # validation data\n"," |    'y_val': # validation labels\n"," |  }\n"," |  model = MyAwesomeModel(hidden_size=100, reg=10)\n"," |  solver = Solver(model, data,\n"," |          update_rule=sgd,\n"," |          optim_config={\n"," |            'learning_rate': 1e-3,\n"," |          },\n"," |          lr_decay=0.95,\n"," |          num_epochs=10, batch_size=100,\n"," |          print_every=100,\n"," |          device='cuda')\n"," |  solver.train()\n"," |  A Solver works on a model object that must conform to the following API:\n"," |  - model.params must be a dictionary mapping string parameter names to torch\n"," |    tensors containing parameter values.\n"," |  - model.loss(X, y) must be a function that computes training-time loss and\n"," |    gradients, and test-time classification scores, with the following inputs\n"," |    and outputs:\n"," |    Inputs:\n"," |    - X: Array giving a minibatch of input data of shape (N, d_1, ..., d_k)\n"," |    - y: Array of labels, of shape (N,) giving labels for X where y[i] is the\n"," |    label for X[i].\n"," |    Returns:\n"," |    If y is None, run a test-time forward pass and return:\n"," |    - scores: Array of shape (N, C) giving classification scores for X where\n"," |    scores[i, c] gives the score of class c for X[i].\n"," |    If y is not None, run a training time forward and backward pass and\n"," |    return a tuple of:\n"," |    - loss: Scalar giving the loss\n"," |    - grads: Dictionary with the same keys as self.params mapping parameter\n"," |    names to gradients of the loss with respect to those parameters.\n"," |    - device: device to use for computation. 'cpu' or 'cuda'\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __init__(self, model, data, **kwargs)\n"," |      Construct a new Solver instance.\n"," |      Required arguments:\n"," |      - model: A model object conforming to the API described above\n"," |      - data: A dictionary of training and validation data containing:\n"," |        'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n"," |        'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n"," |        'y_train': Array, shape (N_train,) of labels for training images\n"," |        'y_val': Array, shape (N_val,) of labels for validation images\n"," |      Optional arguments:\n"," |      - update_rule: A function of an update rule. Default is sgd.\n"," |      - optim_config: A dictionary containing hyperparameters that will be\n"," |        passed to the chosen update rule. Each update rule requires different\n"," |        hyperparameters but all update rules require a\n"," |        'learning_rate' parameter so that should always be present.\n"," |      - lr_decay: A scalar for learning rate decay; after each epoch the\n"," |        learning rate is multiplied by this value.\n"," |      - batch_size: Size of minibatches used to compute loss and gradient\n"," |        during training.\n"," |      - num_epochs: The number of epochs to run for during training.\n"," |      - print_every: Integer; training losses will be printed every\n"," |        print_every iterations.\n"," |      - print_acc_every: We will print the accuracy every\n"," |        print_acc_every epochs.\n"," |      - verbose: Boolean; if set to false then no output will be printed\n"," |        during training.\n"," |      - num_train_samples: Number of training samples used to check training\n"," |        accuracy; default is 1000; set to None to use entire training set.\n"," |      - num_val_samples: Number of validation samples to use to check val\n"," |        accuracy; default is None, which uses the entire validation set.\n"," |      - checkpoint_name: If not None, then save model checkpoints here every\n"," |        epoch.\n"," |  \n"," |  check_accuracy(self, X, y, num_samples=None, batch_size=100)\n"," |      Check accuracy of the model on the provided data.\n"," |      Inputs:\n"," |      - X: Array of data, of shape (N, d_1, ..., d_k)\n"," |      - y: Array of labels, of shape (N,)\n"," |      - num_samples: If not None, subsample the data and only test the model\n"," |        on num_samples datapoints.\n"," |      - batch_size: Split X and y into batches of this size to avoid using\n"," |        too much memory.\n"," |      Returns:\n"," |      - acc: Scalar giving the fraction of instances that were correctly\n"," |        classified by the model.\n"," |  \n"," |  train(self, time_limit=None, return_best_params=True)\n"," |      Run optimization to train the model.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Static methods defined here:\n"," |  \n"," |  sgd(w, dw, config=None)\n"," |      Performs vanilla stochastic gradient descent.\n"," |      config format:\n"," |      - learning_rate: Scalar learning rate.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors defined here:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n","\n","None\n"]}],"source":["print(help(Solver))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"ldQOcCK46YYx","new_sheet":false,"run_control":{"read_only":false}},"source":["Use the Solver classe to create a solver instance that trains a TwoLayerNet to achieve at least 50% performance on the validation set. \n","\n","**Implement** `create_solver_instance` in `fully_connected_networks.py` to return a solver instance. Make sure to initialize the Solver instance with the input device."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"6unJrOule1C_","new_sheet":false,"run_control":{"read_only":false},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677955188242,"user_tz":-180,"elapsed":45219,"user":{"displayName":"Muhammet Can Gümüşsu","userId":"11484611351929850578"}},"outputId":"9f0beda9-a16a-46fd-93e4-0d5cf1847bd5"},"outputs":[{"output_type":"stream","name":"stdout","text":["(Time 0.01 sec; Iteration 1 / 4000) loss: 2.302603\n","(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.092200\n","(Time 0.39 sec; Iteration 11 / 4000) loss: 2.302131\n","(Time 0.44 sec; Iteration 21 / 4000) loss: 2.301974\n","(Time 0.49 sec; Iteration 31 / 4000) loss: 2.302352\n","(Time 0.54 sec; Iteration 41 / 4000) loss: 2.297661\n","(Time 0.60 sec; Iteration 51 / 4000) loss: 2.283626\n","(Time 0.65 sec; Iteration 61 / 4000) loss: 2.268614\n","(Time 0.70 sec; Iteration 71 / 4000) loss: 2.212975\n","(Time 0.75 sec; Iteration 81 / 4000) loss: 2.185241\n","(Time 0.80 sec; Iteration 91 / 4000) loss: 2.173307\n","(Time 0.85 sec; Iteration 101 / 4000) loss: 2.183683\n","(Time 0.90 sec; Iteration 111 / 4000) loss: 2.091396\n","(Time 0.95 sec; Iteration 121 / 4000) loss: 2.059497\n","(Time 1.00 sec; Iteration 131 / 4000) loss: 2.073542\n","(Time 1.05 sec; Iteration 141 / 4000) loss: 2.043060\n","(Time 1.10 sec; Iteration 151 / 4000) loss: 2.057305\n","(Time 1.15 sec; Iteration 161 / 4000) loss: 2.060444\n","(Time 1.20 sec; Iteration 171 / 4000) loss: 1.969404\n","(Time 1.25 sec; Iteration 181 / 4000) loss: 2.018749\n","(Time 1.30 sec; Iteration 191 / 4000) loss: 1.918732\n","(Time 1.35 sec; Iteration 201 / 4000) loss: 1.925371\n","(Time 1.40 sec; Iteration 211 / 4000) loss: 1.986776\n","(Time 1.46 sec; Iteration 221 / 4000) loss: 2.040569\n","(Time 1.51 sec; Iteration 231 / 4000) loss: 1.862027\n","(Time 1.56 sec; Iteration 241 / 4000) loss: 1.882783\n","(Time 1.61 sec; Iteration 251 / 4000) loss: 1.999209\n","(Time 1.66 sec; Iteration 261 / 4000) loss: 1.881364\n","(Time 1.71 sec; Iteration 271 / 4000) loss: 1.869433\n","(Time 1.76 sec; Iteration 281 / 4000) loss: 1.920360\n","(Time 1.81 sec; Iteration 291 / 4000) loss: 1.734718\n","(Time 1.86 sec; Iteration 301 / 4000) loss: 1.746224\n","(Time 1.91 sec; Iteration 311 / 4000) loss: 1.827956\n","(Time 1.96 sec; Iteration 321 / 4000) loss: 1.936580\n","(Time 2.01 sec; Iteration 331 / 4000) loss: 1.991148\n","(Time 2.07 sec; Iteration 341 / 4000) loss: 1.981817\n","(Time 2.12 sec; Iteration 351 / 4000) loss: 1.872227\n","(Time 2.17 sec; Iteration 361 / 4000) loss: 1.688998\n","(Time 2.22 sec; Iteration 371 / 4000) loss: 1.839584\n","(Time 2.27 sec; Iteration 381 / 4000) loss: 1.810971\n","(Time 2.32 sec; Iteration 391 / 4000) loss: 1.808983\n","(Epoch 1 / 10) train acc: 0.361000; val_acc: 0.359000\n","(Time 2.53 sec; Iteration 401 / 4000) loss: 1.866122\n","(Time 2.58 sec; Iteration 411 / 4000) loss: 1.905465\n","(Time 2.63 sec; Iteration 421 / 4000) loss: 1.730916\n","(Time 2.68 sec; Iteration 431 / 4000) loss: 1.752849\n","(Time 2.73 sec; Iteration 441 / 4000) loss: 1.692051\n","(Time 2.78 sec; Iteration 451 / 4000) loss: 1.627449\n","(Time 2.83 sec; Iteration 461 / 4000) loss: 1.766686\n","(Time 2.88 sec; Iteration 471 / 4000) loss: 1.754263\n","(Time 2.93 sec; Iteration 481 / 4000) loss: 1.680517\n","(Time 2.98 sec; Iteration 491 / 4000) loss: 1.819147\n","(Time 3.03 sec; Iteration 501 / 4000) loss: 1.848964\n","(Time 3.08 sec; Iteration 511 / 4000) loss: 1.664147\n","(Time 3.14 sec; Iteration 521 / 4000) loss: 1.861571\n","(Time 3.19 sec; Iteration 531 / 4000) loss: 1.566806\n","(Time 3.24 sec; Iteration 541 / 4000) loss: 1.747006\n","(Time 3.29 sec; Iteration 551 / 4000) loss: 1.634745\n","(Time 3.34 sec; Iteration 561 / 4000) loss: 1.684015\n","(Time 3.39 sec; Iteration 571 / 4000) loss: 1.847447\n","(Time 3.44 sec; Iteration 581 / 4000) loss: 1.796701\n","(Time 3.49 sec; Iteration 591 / 4000) loss: 1.698253\n","(Time 3.54 sec; Iteration 601 / 4000) loss: 1.626178\n","(Time 3.59 sec; Iteration 611 / 4000) loss: 1.793206\n","(Time 3.64 sec; Iteration 621 / 4000) loss: 1.764089\n","(Time 3.69 sec; Iteration 631 / 4000) loss: 1.776800\n","(Time 3.75 sec; Iteration 641 / 4000) loss: 1.736117\n","(Time 3.80 sec; Iteration 651 / 4000) loss: 1.686904\n","(Time 3.85 sec; Iteration 661 / 4000) loss: 1.697896\n","(Time 3.90 sec; Iteration 671 / 4000) loss: 1.786791\n","(Time 3.95 sec; Iteration 681 / 4000) loss: 1.563450\n","(Time 4.00 sec; Iteration 691 / 4000) loss: 1.567511\n","(Time 4.05 sec; Iteration 701 / 4000) loss: 1.760979\n","(Time 4.10 sec; Iteration 711 / 4000) loss: 1.665903\n","(Time 4.15 sec; Iteration 721 / 4000) loss: 1.715495\n","(Time 4.20 sec; Iteration 731 / 4000) loss: 1.478780\n","(Time 4.25 sec; Iteration 741 / 4000) loss: 1.593218\n","(Time 4.30 sec; Iteration 751 / 4000) loss: 1.464391\n","(Time 4.35 sec; Iteration 761 / 4000) loss: 1.576131\n","(Time 4.40 sec; Iteration 771 / 4000) loss: 1.585751\n","(Time 4.45 sec; Iteration 781 / 4000) loss: 1.493866\n","(Time 4.50 sec; Iteration 791 / 4000) loss: 1.659011\n","(Epoch 2 / 10) train acc: 0.429000; val_acc: 0.416400\n","(Time 4.71 sec; Iteration 801 / 4000) loss: 1.522012\n","(Time 4.76 sec; Iteration 811 / 4000) loss: 1.524754\n","(Time 4.81 sec; Iteration 821 / 4000) loss: 1.693718\n","(Time 4.86 sec; Iteration 831 / 4000) loss: 1.619641\n","(Time 4.91 sec; Iteration 841 / 4000) loss: 1.661180\n","(Time 4.96 sec; Iteration 851 / 4000) loss: 1.731999\n","(Time 5.01 sec; Iteration 861 / 4000) loss: 1.388202\n","(Time 5.06 sec; Iteration 871 / 4000) loss: 1.494590\n","(Time 5.11 sec; Iteration 881 / 4000) loss: 1.719062\n","(Time 5.17 sec; Iteration 891 / 4000) loss: 1.696230\n","(Time 5.22 sec; Iteration 901 / 4000) loss: 1.720399\n","(Time 5.26 sec; Iteration 911 / 4000) loss: 1.634952\n","(Time 5.31 sec; Iteration 921 / 4000) loss: 1.641377\n","(Time 5.36 sec; Iteration 931 / 4000) loss: 1.613847\n","(Time 5.41 sec; Iteration 941 / 4000) loss: 1.416605\n","(Time 5.46 sec; Iteration 951 / 4000) loss: 1.493810\n","(Time 5.51 sec; Iteration 961 / 4000) loss: 1.609787\n","(Time 5.56 sec; Iteration 971 / 4000) loss: 1.434485\n","(Time 5.62 sec; Iteration 981 / 4000) loss: 1.673444\n","(Time 5.67 sec; Iteration 991 / 4000) loss: 1.492794\n","(Time 5.72 sec; Iteration 1001 / 4000) loss: 1.417730\n","(Time 5.77 sec; Iteration 1011 / 4000) loss: 1.515685\n","(Time 5.82 sec; Iteration 1021 / 4000) loss: 1.690853\n","(Time 5.87 sec; Iteration 1031 / 4000) loss: 1.463434\n","(Time 5.92 sec; Iteration 1041 / 4000) loss: 1.534649\n","(Time 5.97 sec; Iteration 1051 / 4000) loss: 1.518368\n","(Time 6.02 sec; Iteration 1061 / 4000) loss: 1.572649\n","(Time 6.07 sec; Iteration 1071 / 4000) loss: 1.496677\n","(Time 6.12 sec; Iteration 1081 / 4000) loss: 1.467277\n","(Time 6.17 sec; Iteration 1091 / 4000) loss: 1.411377\n","(Time 6.22 sec; Iteration 1101 / 4000) loss: 1.508326\n","(Time 6.27 sec; Iteration 1111 / 4000) loss: 1.565717\n","(Time 6.32 sec; Iteration 1121 / 4000) loss: 1.502134\n","(Time 6.37 sec; Iteration 1131 / 4000) loss: 1.524817\n","(Time 6.42 sec; Iteration 1141 / 4000) loss: 1.462300\n","(Time 6.47 sec; Iteration 1151 / 4000) loss: 1.596607\n","(Time 6.53 sec; Iteration 1161 / 4000) loss: 1.412042\n","(Time 6.58 sec; Iteration 1171 / 4000) loss: 1.525309\n","(Time 6.62 sec; Iteration 1181 / 4000) loss: 1.389256\n","(Time 6.68 sec; Iteration 1191 / 4000) loss: 1.458935\n","(Epoch 3 / 10) train acc: 0.447000; val_acc: 0.441400\n","(Time 6.89 sec; Iteration 1201 / 4000) loss: 1.573176\n","(Time 6.94 sec; Iteration 1211 / 4000) loss: 1.467197\n","(Time 6.99 sec; Iteration 1221 / 4000) loss: 1.607835\n","(Time 7.04 sec; Iteration 1231 / 4000) loss: 1.537023\n","(Time 7.09 sec; Iteration 1241 / 4000) loss: 1.591737\n","(Time 7.14 sec; Iteration 1251 / 4000) loss: 1.525906\n","(Time 7.19 sec; Iteration 1261 / 4000) loss: 1.488748\n","(Time 7.24 sec; Iteration 1271 / 4000) loss: 1.342857\n","(Time 7.29 sec; Iteration 1281 / 4000) loss: 1.487766\n","(Time 7.34 sec; Iteration 1291 / 4000) loss: 1.296228\n","(Time 7.39 sec; Iteration 1301 / 4000) loss: 1.415928\n","(Time 7.44 sec; Iteration 1311 / 4000) loss: 1.487382\n","(Time 7.50 sec; Iteration 1321 / 4000) loss: 1.467810\n","(Time 7.55 sec; Iteration 1331 / 4000) loss: 1.560782\n","(Time 7.60 sec; Iteration 1341 / 4000) loss: 1.538561\n","(Time 7.65 sec; Iteration 1351 / 4000) loss: 1.659575\n","(Time 7.70 sec; Iteration 1361 / 4000) loss: 1.516713\n","(Time 7.75 sec; Iteration 1371 / 4000) loss: 1.435643\n","(Time 7.80 sec; Iteration 1381 / 4000) loss: 1.490255\n","(Time 7.85 sec; Iteration 1391 / 4000) loss: 1.615745\n","(Time 7.90 sec; Iteration 1401 / 4000) loss: 1.681612\n","(Time 7.95 sec; Iteration 1411 / 4000) loss: 1.311217\n","(Time 8.00 sec; Iteration 1421 / 4000) loss: 1.531865\n","(Time 8.05 sec; Iteration 1431 / 4000) loss: 1.448577\n","(Time 8.10 sec; Iteration 1441 / 4000) loss: 1.437073\n","(Time 8.15 sec; Iteration 1451 / 4000) loss: 1.616219\n","(Time 8.20 sec; Iteration 1461 / 4000) loss: 1.583338\n","(Time 8.25 sec; Iteration 1471 / 4000) loss: 1.532556\n","(Time 8.30 sec; Iteration 1481 / 4000) loss: 1.563625\n","(Time 8.35 sec; Iteration 1491 / 4000) loss: 1.614664\n","(Time 8.40 sec; Iteration 1501 / 4000) loss: 1.386953\n","(Time 8.45 sec; Iteration 1511 / 4000) loss: 1.421970\n","(Time 8.50 sec; Iteration 1521 / 4000) loss: 1.266908\n","(Time 8.55 sec; Iteration 1531 / 4000) loss: 1.407091\n","(Time 8.60 sec; Iteration 1541 / 4000) loss: 1.641110\n","(Time 8.65 sec; Iteration 1551 / 4000) loss: 1.633930\n","(Time 8.71 sec; Iteration 1561 / 4000) loss: 1.402136\n","(Time 8.77 sec; Iteration 1571 / 4000) loss: 1.354367\n","(Time 8.83 sec; Iteration 1581 / 4000) loss: 1.241307\n","(Time 8.88 sec; Iteration 1591 / 4000) loss: 1.481929\n","(Epoch 4 / 10) train acc: 0.463000; val_acc: 0.459100\n","(Time 9.10 sec; Iteration 1601 / 4000) loss: 1.521766\n","(Time 9.16 sec; Iteration 1611 / 4000) loss: 1.242913\n","(Time 9.21 sec; Iteration 1621 / 4000) loss: 1.367333\n","(Time 9.27 sec; Iteration 1631 / 4000) loss: 1.551271\n","(Time 9.33 sec; Iteration 1641 / 4000) loss: 1.547451\n","(Time 9.38 sec; Iteration 1651 / 4000) loss: 1.404027\n","(Time 9.44 sec; Iteration 1661 / 4000) loss: 1.451265\n","(Time 9.49 sec; Iteration 1671 / 4000) loss: 1.512043\n","(Time 9.56 sec; Iteration 1681 / 4000) loss: 1.411674\n","(Time 9.61 sec; Iteration 1691 / 4000) loss: 1.390853\n","(Time 9.67 sec; Iteration 1701 / 4000) loss: 1.335102\n","(Time 9.73 sec; Iteration 1711 / 4000) loss: 1.486694\n","(Time 9.79 sec; Iteration 1721 / 4000) loss: 1.555205\n","(Time 9.84 sec; Iteration 1731 / 4000) loss: 1.563746\n","(Time 9.90 sec; Iteration 1741 / 4000) loss: 1.477284\n","(Time 9.95 sec; Iteration 1751 / 4000) loss: 1.505864\n","(Time 10.01 sec; Iteration 1761 / 4000) loss: 1.271154\n","(Time 10.07 sec; Iteration 1771 / 4000) loss: 1.525193\n","(Time 10.12 sec; Iteration 1781 / 4000) loss: 1.389323\n","(Time 10.18 sec; Iteration 1791 / 4000) loss: 1.468372\n","(Time 10.24 sec; Iteration 1801 / 4000) loss: 1.417168\n","(Time 10.30 sec; Iteration 1811 / 4000) loss: 1.416766\n","(Time 10.35 sec; Iteration 1821 / 4000) loss: 1.508289\n","(Time 10.40 sec; Iteration 1831 / 4000) loss: 1.426444\n","(Time 10.47 sec; Iteration 1841 / 4000) loss: 1.395155\n","(Time 10.52 sec; Iteration 1851 / 4000) loss: 1.336724\n","(Time 10.58 sec; Iteration 1861 / 4000) loss: 1.288793\n","(Time 10.63 sec; Iteration 1871 / 4000) loss: 1.305221\n","(Time 10.69 sec; Iteration 1881 / 4000) loss: 1.337113\n","(Time 10.75 sec; Iteration 1891 / 4000) loss: 1.498912\n","(Time 10.80 sec; Iteration 1901 / 4000) loss: 1.364826\n","(Time 10.86 sec; Iteration 1911 / 4000) loss: 1.266581\n","(Time 10.92 sec; Iteration 1921 / 4000) loss: 1.352975\n","(Time 10.97 sec; Iteration 1931 / 4000) loss: 1.417407\n","(Time 11.03 sec; Iteration 1941 / 4000) loss: 1.237848\n","(Time 11.08 sec; Iteration 1951 / 4000) loss: 1.352375\n","(Time 11.14 sec; Iteration 1961 / 4000) loss: 1.264502\n","(Time 11.20 sec; Iteration 1971 / 4000) loss: 1.476294\n","(Time 11.25 sec; Iteration 1981 / 4000) loss: 1.172755\n","(Time 11.31 sec; Iteration 1991 / 4000) loss: 1.470435\n","(Epoch 5 / 10) train acc: 0.517000; val_acc: 0.474700\n","(Time 11.53 sec; Iteration 2001 / 4000) loss: 1.412173\n","(Time 11.59 sec; Iteration 2011 / 4000) loss: 1.365631\n","(Time 11.64 sec; Iteration 2021 / 4000) loss: 1.380066\n","(Time 11.70 sec; Iteration 2031 / 4000) loss: 1.388442\n","(Time 11.76 sec; Iteration 2041 / 4000) loss: 1.406061\n","(Time 11.82 sec; Iteration 2051 / 4000) loss: 1.527548\n","(Time 11.87 sec; Iteration 2061 / 4000) loss: 1.423839\n","(Time 11.93 sec; Iteration 2071 / 4000) loss: 1.351541\n","(Time 11.99 sec; Iteration 2081 / 4000) loss: 1.392506\n","(Time 12.04 sec; Iteration 2091 / 4000) loss: 1.415820\n","(Time 12.09 sec; Iteration 2101 / 4000) loss: 1.368111\n","(Time 12.14 sec; Iteration 2111 / 4000) loss: 1.495955\n","(Time 12.19 sec; Iteration 2121 / 4000) loss: 1.439826\n","(Time 12.24 sec; Iteration 2131 / 4000) loss: 1.424299\n","(Time 12.29 sec; Iteration 2141 / 4000) loss: 1.480144\n","(Time 12.34 sec; Iteration 2151 / 4000) loss: 1.535295\n","(Time 12.39 sec; Iteration 2161 / 4000) loss: 1.348136\n","(Time 12.44 sec; Iteration 2171 / 4000) loss: 1.192357\n","(Time 12.49 sec; Iteration 2181 / 4000) loss: 1.474535\n","(Time 12.54 sec; Iteration 2191 / 4000) loss: 1.315101\n","(Time 12.59 sec; Iteration 2201 / 4000) loss: 1.386365\n","(Time 12.64 sec; Iteration 2211 / 4000) loss: 1.292390\n","(Time 12.69 sec; Iteration 2221 / 4000) loss: 1.374970\n","(Time 12.74 sec; Iteration 2231 / 4000) loss: 1.224959\n","(Time 12.79 sec; Iteration 2241 / 4000) loss: 1.395312\n","(Time 12.84 sec; Iteration 2251 / 4000) loss: 1.419005\n","(Time 12.89 sec; Iteration 2261 / 4000) loss: 1.312549\n","(Time 12.94 sec; Iteration 2271 / 4000) loss: 1.373220\n","(Time 12.99 sec; Iteration 2281 / 4000) loss: 1.416466\n","(Time 13.04 sec; Iteration 2291 / 4000) loss: 1.237260\n","(Time 13.09 sec; Iteration 2301 / 4000) loss: 1.500228\n","(Time 13.14 sec; Iteration 2311 / 4000) loss: 1.371111\n","(Time 13.19 sec; Iteration 2321 / 4000) loss: 1.398013\n","(Time 13.24 sec; Iteration 2331 / 4000) loss: 1.454128\n","(Time 13.29 sec; Iteration 2341 / 4000) loss: 1.293662\n","(Time 13.34 sec; Iteration 2351 / 4000) loss: 1.318695\n","(Time 13.39 sec; Iteration 2361 / 4000) loss: 1.328741\n","(Time 13.44 sec; Iteration 2371 / 4000) loss: 1.452432\n","(Time 13.49 sec; Iteration 2381 / 4000) loss: 1.607329\n","(Time 13.54 sec; Iteration 2391 / 4000) loss: 1.353413\n","(Epoch 6 / 10) train acc: 0.549000; val_acc: 0.486500\n","(Time 13.76 sec; Iteration 2401 / 4000) loss: 1.341348\n","(Time 13.81 sec; Iteration 2411 / 4000) loss: 1.454511\n","(Time 13.86 sec; Iteration 2421 / 4000) loss: 1.530261\n","(Time 13.91 sec; Iteration 2431 / 4000) loss: 1.269506\n","(Time 13.96 sec; Iteration 2441 / 4000) loss: 1.215866\n","(Time 14.01 sec; Iteration 2451 / 4000) loss: 1.234394\n","(Time 14.06 sec; Iteration 2461 / 4000) loss: 1.497389\n","(Time 14.11 sec; Iteration 2471 / 4000) loss: 1.437315\n","(Time 14.16 sec; Iteration 2481 / 4000) loss: 1.112067\n","(Time 14.21 sec; Iteration 2491 / 4000) loss: 1.571989\n","(Time 14.26 sec; Iteration 2501 / 4000) loss: 1.347804\n","(Time 14.31 sec; Iteration 2511 / 4000) loss: 1.473773\n","(Time 14.36 sec; Iteration 2521 / 4000) loss: 1.311973\n","(Time 14.41 sec; Iteration 2531 / 4000) loss: 1.300632\n","(Time 14.46 sec; Iteration 2541 / 4000) loss: 1.323950\n","(Time 14.51 sec; Iteration 2551 / 4000) loss: 1.197390\n","(Time 14.56 sec; Iteration 2561 / 4000) loss: 1.297808\n","(Time 14.61 sec; Iteration 2571 / 4000) loss: 1.443908\n","(Time 14.66 sec; Iteration 2581 / 4000) loss: 1.362915\n","(Time 14.71 sec; Iteration 2591 / 4000) loss: 1.387469\n","(Time 14.76 sec; Iteration 2601 / 4000) loss: 1.197494\n","(Time 14.81 sec; Iteration 2611 / 4000) loss: 1.331915\n","(Time 14.86 sec; Iteration 2621 / 4000) loss: 1.407377\n","(Time 14.91 sec; Iteration 2631 / 4000) loss: 1.348649\n","(Time 14.96 sec; Iteration 2641 / 4000) loss: 1.522015\n","(Time 15.01 sec; Iteration 2651 / 4000) loss: 1.474020\n","(Time 15.06 sec; Iteration 2661 / 4000) loss: 1.439386\n","(Time 15.11 sec; Iteration 2671 / 4000) loss: 1.443060\n","(Time 15.16 sec; Iteration 2681 / 4000) loss: 1.181620\n","(Time 15.21 sec; Iteration 2691 / 4000) loss: 1.268568\n","(Time 15.27 sec; Iteration 2701 / 4000) loss: 1.278015\n","(Time 15.32 sec; Iteration 2711 / 4000) loss: 1.255225\n","(Time 15.37 sec; Iteration 2721 / 4000) loss: 1.162669\n","(Time 15.42 sec; Iteration 2731 / 4000) loss: 1.227514\n","(Time 15.47 sec; Iteration 2741 / 4000) loss: 1.356799\n","(Time 15.52 sec; Iteration 2751 / 4000) loss: 1.235003\n","(Time 15.56 sec; Iteration 2761 / 4000) loss: 1.399809\n","(Time 15.62 sec; Iteration 2771 / 4000) loss: 1.254974\n","(Time 15.67 sec; Iteration 2781 / 4000) loss: 1.417478\n","(Time 15.72 sec; Iteration 2791 / 4000) loss: 1.201602\n","(Epoch 7 / 10) train acc: 0.539000; val_acc: 0.489900\n","(Time 15.93 sec; Iteration 2801 / 4000) loss: 1.325908\n","(Time 15.98 sec; Iteration 2811 / 4000) loss: 1.259086\n","(Time 16.03 sec; Iteration 2821 / 4000) loss: 1.363871\n","(Time 16.08 sec; Iteration 2831 / 4000) loss: 1.341692\n","(Time 16.13 sec; Iteration 2841 / 4000) loss: 1.180890\n","(Time 16.18 sec; Iteration 2851 / 4000) loss: 1.124726\n","(Time 16.23 sec; Iteration 2861 / 4000) loss: 1.398927\n","(Time 16.28 sec; Iteration 2871 / 4000) loss: 1.309478\n","(Time 16.33 sec; Iteration 2881 / 4000) loss: 1.401368\n","(Time 16.38 sec; Iteration 2891 / 4000) loss: 1.377041\n","(Time 16.43 sec; Iteration 2901 / 4000) loss: 1.318557\n","(Time 16.48 sec; Iteration 2911 / 4000) loss: 1.292226\n","(Time 16.53 sec; Iteration 2921 / 4000) loss: 1.508426\n","(Time 16.58 sec; Iteration 2931 / 4000) loss: 1.409223\n","(Time 16.63 sec; Iteration 2941 / 4000) loss: 1.149919\n","(Time 16.68 sec; Iteration 2951 / 4000) loss: 1.287884\n","(Time 16.73 sec; Iteration 2961 / 4000) loss: 1.353141\n","(Time 16.78 sec; Iteration 2971 / 4000) loss: 1.368842\n","(Time 16.83 sec; Iteration 2981 / 4000) loss: 1.188245\n","(Time 16.88 sec; Iteration 2991 / 4000) loss: 1.343926\n","(Time 16.93 sec; Iteration 3001 / 4000) loss: 1.470120\n","(Time 16.98 sec; Iteration 3011 / 4000) loss: 1.311723\n","(Time 17.03 sec; Iteration 3021 / 4000) loss: 1.390933\n","(Time 17.08 sec; Iteration 3031 / 4000) loss: 1.181671\n","(Time 17.13 sec; Iteration 3041 / 4000) loss: 1.360569\n","(Time 17.18 sec; Iteration 3051 / 4000) loss: 1.254770\n","(Time 17.23 sec; Iteration 3061 / 4000) loss: 1.226239\n","(Time 17.28 sec; Iteration 3071 / 4000) loss: 1.171366\n","(Time 17.33 sec; Iteration 3081 / 4000) loss: 1.206862\n","(Time 17.38 sec; Iteration 3091 / 4000) loss: 1.231305\n","(Time 17.43 sec; Iteration 3101 / 4000) loss: 1.246286\n","(Time 17.48 sec; Iteration 3111 / 4000) loss: 1.442142\n","(Time 17.53 sec; Iteration 3121 / 4000) loss: 1.204228\n","(Time 17.58 sec; Iteration 3131 / 4000) loss: 1.289332\n","(Time 17.63 sec; Iteration 3141 / 4000) loss: 1.269922\n","(Time 17.68 sec; Iteration 3151 / 4000) loss: 1.245389\n","(Time 17.73 sec; Iteration 3161 / 4000) loss: 1.221220\n","(Time 17.78 sec; Iteration 3171 / 4000) loss: 1.161363\n","(Time 17.83 sec; Iteration 3181 / 4000) loss: 1.381036\n","(Time 17.88 sec; Iteration 3191 / 4000) loss: 1.286729\n","(Epoch 8 / 10) train acc: 0.529000; val_acc: 0.496400\n","(Time 18.10 sec; Iteration 3201 / 4000) loss: 1.180377\n","(Time 18.15 sec; Iteration 3211 / 4000) loss: 1.409708\n","(Time 18.20 sec; Iteration 3221 / 4000) loss: 1.242363\n","(Time 18.25 sec; Iteration 3231 / 4000) loss: 1.109249\n","(Time 18.30 sec; Iteration 3241 / 4000) loss: 1.294702\n","(Time 18.35 sec; Iteration 3251 / 4000) loss: 1.317802\n","(Time 18.40 sec; Iteration 3261 / 4000) loss: 1.347546\n","(Time 18.45 sec; Iteration 3271 / 4000) loss: 1.235995\n","(Time 18.50 sec; Iteration 3281 / 4000) loss: 1.380530\n","(Time 18.55 sec; Iteration 3291 / 4000) loss: 1.381805\n","(Time 18.61 sec; Iteration 3301 / 4000) loss: 1.360922\n","(Time 18.66 sec; Iteration 3311 / 4000) loss: 1.268379\n","(Time 18.71 sec; Iteration 3321 / 4000) loss: 1.218190\n","(Time 18.76 sec; Iteration 3331 / 4000) loss: 1.279020\n","(Time 18.81 sec; Iteration 3341 / 4000) loss: 1.219954\n","(Time 18.86 sec; Iteration 3351 / 4000) loss: 1.212037\n","(Time 18.91 sec; Iteration 3361 / 4000) loss: 1.360056\n","(Time 18.96 sec; Iteration 3371 / 4000) loss: 1.275914\n","(Time 19.01 sec; Iteration 3381 / 4000) loss: 1.101094\n","(Time 19.06 sec; Iteration 3391 / 4000) loss: 1.436274\n","(Time 19.11 sec; Iteration 3401 / 4000) loss: 1.390154\n","(Time 19.16 sec; Iteration 3411 / 4000) loss: 1.142683\n","(Time 19.21 sec; Iteration 3421 / 4000) loss: 1.312863\n","(Time 19.26 sec; Iteration 3431 / 4000) loss: 1.263322\n","(Time 19.31 sec; Iteration 3441 / 4000) loss: 1.453143\n","(Time 19.36 sec; Iteration 3451 / 4000) loss: 1.384671\n","(Time 19.41 sec; Iteration 3461 / 4000) loss: 1.509826\n","(Time 19.46 sec; Iteration 3471 / 4000) loss: 1.198599\n","(Time 19.51 sec; Iteration 3481 / 4000) loss: 1.238305\n","(Time 19.56 sec; Iteration 3491 / 4000) loss: 1.242721\n","(Time 19.61 sec; Iteration 3501 / 4000) loss: 1.126090\n","(Time 19.66 sec; Iteration 3511 / 4000) loss: 1.109522\n","(Time 19.71 sec; Iteration 3521 / 4000) loss: 1.393798\n","(Time 19.76 sec; Iteration 3531 / 4000) loss: 1.260682\n","(Time 19.81 sec; Iteration 3541 / 4000) loss: 1.296079\n","(Time 19.86 sec; Iteration 3551 / 4000) loss: 1.252848\n","(Time 19.91 sec; Iteration 3561 / 4000) loss: 1.365798\n","(Time 19.96 sec; Iteration 3571 / 4000) loss: 1.337792\n","(Time 20.01 sec; Iteration 3581 / 4000) loss: 1.396553\n","(Time 20.06 sec; Iteration 3591 / 4000) loss: 1.414233\n","(Epoch 9 / 10) train acc: 0.568000; val_acc: 0.506900\n","(Time 20.27 sec; Iteration 3601 / 4000) loss: 1.471752\n","(Time 20.32 sec; Iteration 3611 / 4000) loss: 1.306402\n","(Time 20.37 sec; Iteration 3621 / 4000) loss: 1.383040\n","(Time 20.42 sec; Iteration 3631 / 4000) loss: 1.384934\n","(Time 20.47 sec; Iteration 3641 / 4000) loss: 1.227346\n","(Time 20.52 sec; Iteration 3651 / 4000) loss: 1.154861\n","(Time 20.57 sec; Iteration 3661 / 4000) loss: 1.203838\n","(Time 20.62 sec; Iteration 3671 / 4000) loss: 1.268199\n","(Time 20.67 sec; Iteration 3681 / 4000) loss: 1.264610\n","(Time 20.72 sec; Iteration 3691 / 4000) loss: 1.090994\n","(Time 20.77 sec; Iteration 3701 / 4000) loss: 1.214736\n","(Time 20.82 sec; Iteration 3711 / 4000) loss: 1.398486\n","(Time 20.87 sec; Iteration 3721 / 4000) loss: 1.376327\n","(Time 20.92 sec; Iteration 3731 / 4000) loss: 1.272982\n","(Time 20.97 sec; Iteration 3741 / 4000) loss: 1.278452\n","(Time 21.02 sec; Iteration 3751 / 4000) loss: 1.165819\n","(Time 21.07 sec; Iteration 3761 / 4000) loss: 1.259966\n","(Time 21.12 sec; Iteration 3771 / 4000) loss: 1.371037\n","(Time 21.17 sec; Iteration 3781 / 4000) loss: 1.139055\n","(Time 21.23 sec; Iteration 3791 / 4000) loss: 1.253244\n","(Time 21.28 sec; Iteration 3801 / 4000) loss: 1.424030\n","(Time 21.33 sec; Iteration 3811 / 4000) loss: 1.281586\n","(Time 21.38 sec; Iteration 3821 / 4000) loss: 1.373167\n","(Time 21.43 sec; Iteration 3831 / 4000) loss: 1.254153\n","(Time 21.48 sec; Iteration 3841 / 4000) loss: 1.246466\n","(Time 21.53 sec; Iteration 3851 / 4000) loss: 1.205352\n","(Time 21.58 sec; Iteration 3861 / 4000) loss: 1.563120\n","(Time 21.63 sec; Iteration 3871 / 4000) loss: 1.237035\n","(Time 21.68 sec; Iteration 3881 / 4000) loss: 1.333726\n","(Time 21.73 sec; Iteration 3891 / 4000) loss: 1.190968\n","(Time 21.78 sec; Iteration 3901 / 4000) loss: 1.442500\n","(Time 21.83 sec; Iteration 3911 / 4000) loss: 1.306660\n","(Time 21.88 sec; Iteration 3921 / 4000) loss: 1.037206\n","(Time 21.93 sec; Iteration 3931 / 4000) loss: 1.385250\n","(Time 21.98 sec; Iteration 3941 / 4000) loss: 1.100189\n","(Time 22.04 sec; Iteration 3951 / 4000) loss: 1.295998\n","(Time 22.10 sec; Iteration 3961 / 4000) loss: 1.167010\n","(Time 22.16 sec; Iteration 3971 / 4000) loss: 1.510381\n","(Time 22.21 sec; Iteration 3981 / 4000) loss: 1.384203\n","(Time 22.27 sec; Iteration 3991 / 4000) loss: 1.292444\n","(Epoch 10 / 10) train acc: 0.581000; val_acc: 0.506100\n","(Time 0.01 sec; Iteration 1 / 4000) loss: 1.254120\n","(Epoch 10 / 10) train acc: 0.563000; val_acc: 0.507300\n","(Time 0.23 sec; Iteration 11 / 4000) loss: 1.426951\n","(Time 0.28 sec; Iteration 21 / 4000) loss: 1.379503\n","(Time 0.34 sec; Iteration 31 / 4000) loss: 1.359155\n","(Time 0.39 sec; Iteration 41 / 4000) loss: 1.275139\n","(Time 0.45 sec; Iteration 51 / 4000) loss: 1.348644\n","(Time 0.50 sec; Iteration 61 / 4000) loss: 1.214349\n","(Time 0.56 sec; Iteration 71 / 4000) loss: 1.225228\n","(Time 0.62 sec; Iteration 81 / 4000) loss: 1.230092\n","(Time 0.67 sec; Iteration 91 / 4000) loss: 1.352940\n","(Time 0.73 sec; Iteration 101 / 4000) loss: 1.353351\n","(Time 0.80 sec; Iteration 111 / 4000) loss: 1.168833\n","(Time 0.85 sec; Iteration 121 / 4000) loss: 1.309949\n","(Time 0.91 sec; Iteration 131 / 4000) loss: 1.198961\n","(Time 0.97 sec; Iteration 141 / 4000) loss: 1.170285\n","(Time 1.02 sec; Iteration 151 / 4000) loss: 1.314556\n","(Time 1.08 sec; Iteration 161 / 4000) loss: 1.171324\n","(Time 1.13 sec; Iteration 171 / 4000) loss: 1.128881\n","(Time 1.19 sec; Iteration 181 / 4000) loss: 1.253205\n","(Time 1.25 sec; Iteration 191 / 4000) loss: 1.304266\n","(Time 1.30 sec; Iteration 201 / 4000) loss: 1.244481\n","(Time 1.36 sec; Iteration 211 / 4000) loss: 1.155058\n","(Time 1.42 sec; Iteration 221 / 4000) loss: 1.208202\n","(Time 1.47 sec; Iteration 231 / 4000) loss: 1.230965\n","(Time 1.53 sec; Iteration 241 / 4000) loss: 1.227278\n","(Time 1.58 sec; Iteration 251 / 4000) loss: 1.304964\n","(Time 1.64 sec; Iteration 261 / 4000) loss: 1.384426\n","(Time 1.69 sec; Iteration 271 / 4000) loss: 1.137794\n","(Time 1.75 sec; Iteration 281 / 4000) loss: 1.199572\n","(Time 1.81 sec; Iteration 291 / 4000) loss: 1.233331\n","(Time 1.87 sec; Iteration 301 / 4000) loss: 1.189942\n","(Time 1.93 sec; Iteration 311 / 4000) loss: 1.437139\n","(Time 1.99 sec; Iteration 321 / 4000) loss: 1.126551\n","(Time 2.05 sec; Iteration 331 / 4000) loss: 1.132828\n","(Time 2.10 sec; Iteration 341 / 4000) loss: 1.182145\n","(Time 2.16 sec; Iteration 351 / 4000) loss: 1.203717\n","(Time 2.21 sec; Iteration 361 / 4000) loss: 1.193405\n","(Time 2.27 sec; Iteration 371 / 4000) loss: 1.293777\n","(Time 2.32 sec; Iteration 381 / 4000) loss: 1.025663\n","(Time 2.38 sec; Iteration 391 / 4000) loss: 1.134689\n","(Epoch 11 / 10) train acc: 0.568000; val_acc: 0.509200\n","(Time 2.59 sec; Iteration 401 / 4000) loss: 1.272397\n","(Time 2.65 sec; Iteration 411 / 4000) loss: 1.244022\n","(Time 2.71 sec; Iteration 421 / 4000) loss: 1.215689\n","(Time 2.76 sec; Iteration 431 / 4000) loss: 1.054409\n","(Time 2.82 sec; Iteration 441 / 4000) loss: 1.233457\n","(Time 2.88 sec; Iteration 451 / 4000) loss: 1.149012\n","(Time 2.93 sec; Iteration 461 / 4000) loss: 1.414060\n","(Time 2.99 sec; Iteration 471 / 4000) loss: 1.339973\n","(Time 3.04 sec; Iteration 481 / 4000) loss: 1.179199\n","(Time 3.09 sec; Iteration 491 / 4000) loss: 1.212560\n","(Time 3.14 sec; Iteration 501 / 4000) loss: 1.283934\n","(Time 3.19 sec; Iteration 511 / 4000) loss: 1.334360\n","(Time 3.24 sec; Iteration 521 / 4000) loss: 1.328664\n","(Time 3.29 sec; Iteration 531 / 4000) loss: 1.259842\n","(Time 3.34 sec; Iteration 541 / 4000) loss: 1.185782\n","(Time 3.39 sec; Iteration 551 / 4000) loss: 1.286964\n","(Time 3.44 sec; Iteration 561 / 4000) loss: 1.233065\n","(Time 3.49 sec; Iteration 571 / 4000) loss: 1.105361\n","(Time 3.54 sec; Iteration 581 / 4000) loss: 1.260059\n","(Time 3.59 sec; Iteration 591 / 4000) loss: 1.378258\n","(Time 3.64 sec; Iteration 601 / 4000) loss: 1.206076\n","(Time 3.69 sec; Iteration 611 / 4000) loss: 1.142224\n","(Time 3.74 sec; Iteration 621 / 4000) loss: 1.138461\n","(Time 3.79 sec; Iteration 631 / 4000) loss: 1.222408\n","(Time 3.84 sec; Iteration 641 / 4000) loss: 1.213934\n","(Time 3.89 sec; Iteration 651 / 4000) loss: 1.217775\n","(Time 3.95 sec; Iteration 661 / 4000) loss: 1.134885\n","(Time 4.00 sec; Iteration 671 / 4000) loss: 1.158124\n","(Time 4.05 sec; Iteration 681 / 4000) loss: 1.224704\n","(Time 4.10 sec; Iteration 691 / 4000) loss: 1.330897\n","(Time 4.15 sec; Iteration 701 / 4000) loss: 1.218021\n","(Time 4.20 sec; Iteration 711 / 4000) loss: 1.319299\n","(Time 4.25 sec; Iteration 721 / 4000) loss: 1.195826\n","(Time 4.30 sec; Iteration 731 / 4000) loss: 1.272272\n","(Time 4.35 sec; Iteration 741 / 4000) loss: 1.101790\n","(Time 4.40 sec; Iteration 751 / 4000) loss: 1.248954\n","(Time 4.45 sec; Iteration 761 / 4000) loss: 1.283492\n","(Time 4.50 sec; Iteration 771 / 4000) loss: 1.199551\n","(Time 4.55 sec; Iteration 781 / 4000) loss: 1.360402\n","(Time 4.60 sec; Iteration 791 / 4000) loss: 1.173442\n","(Epoch 12 / 10) train acc: 0.568000; val_acc: 0.509600\n","(Time 4.81 sec; Iteration 801 / 4000) loss: 1.205088\n","(Time 4.86 sec; Iteration 811 / 4000) loss: 1.320710\n","(Time 4.91 sec; Iteration 821 / 4000) loss: 1.122924\n","(Time 4.96 sec; Iteration 831 / 4000) loss: 1.105449\n","(Time 5.01 sec; Iteration 841 / 4000) loss: 1.195156\n","(Time 5.06 sec; Iteration 851 / 4000) loss: 1.143026\n","(Time 5.11 sec; Iteration 861 / 4000) loss: 1.219143\n","(Time 5.16 sec; Iteration 871 / 4000) loss: 1.115492\n","(Time 5.21 sec; Iteration 881 / 4000) loss: 1.290097\n","(Time 5.26 sec; Iteration 891 / 4000) loss: 1.145388\n","(Time 5.31 sec; Iteration 901 / 4000) loss: 1.195312\n","(Time 5.36 sec; Iteration 911 / 4000) loss: 1.267227\n","(Time 5.41 sec; Iteration 921 / 4000) loss: 1.099499\n","(Time 5.46 sec; Iteration 931 / 4000) loss: 1.100510\n","(Time 5.51 sec; Iteration 941 / 4000) loss: 1.213779\n","(Time 5.56 sec; Iteration 951 / 4000) loss: 1.229775\n","(Time 5.61 sec; Iteration 961 / 4000) loss: 1.229266\n","(Time 5.66 sec; Iteration 971 / 4000) loss: 1.354919\n","(Time 5.71 sec; Iteration 981 / 4000) loss: 1.254275\n","(Time 5.76 sec; Iteration 991 / 4000) loss: 1.188621\n","(Time 5.81 sec; Iteration 1001 / 4000) loss: 1.133323\n","(Time 5.86 sec; Iteration 1011 / 4000) loss: 1.198950\n","(Time 5.91 sec; Iteration 1021 / 4000) loss: 1.192447\n","(Time 5.96 sec; Iteration 1031 / 4000) loss: 1.283163\n","(Time 6.01 sec; Iteration 1041 / 4000) loss: 1.253640\n","(Time 6.07 sec; Iteration 1051 / 4000) loss: 0.998969\n","(Time 6.11 sec; Iteration 1061 / 4000) loss: 1.342565\n","(Time 6.17 sec; Iteration 1071 / 4000) loss: 1.176249\n","(Time 6.22 sec; Iteration 1081 / 4000) loss: 1.175599\n","(Time 6.27 sec; Iteration 1091 / 4000) loss: 1.209268\n","(Time 6.32 sec; Iteration 1101 / 4000) loss: 1.175370\n","(Time 6.37 sec; Iteration 1111 / 4000) loss: 1.371707\n","(Time 6.42 sec; Iteration 1121 / 4000) loss: 1.213224\n","(Time 6.47 sec; Iteration 1131 / 4000) loss: 1.122769\n","(Time 6.52 sec; Iteration 1141 / 4000) loss: 1.213509\n","(Time 6.57 sec; Iteration 1151 / 4000) loss: 1.059777\n","(Time 6.62 sec; Iteration 1161 / 4000) loss: 1.159585\n","(Time 6.67 sec; Iteration 1171 / 4000) loss: 1.254459\n","(Time 6.72 sec; Iteration 1181 / 4000) loss: 1.282117\n","(Time 6.77 sec; Iteration 1191 / 4000) loss: 1.197780\n","(Epoch 13 / 10) train acc: 0.552000; val_acc: 0.513900\n","(Time 6.98 sec; Iteration 1201 / 4000) loss: 1.247510\n","(Time 7.03 sec; Iteration 1211 / 4000) loss: 0.962679\n","(Time 7.08 sec; Iteration 1221 / 4000) loss: 1.145489\n","(Time 7.13 sec; Iteration 1231 / 4000) loss: 1.232360\n","(Time 7.18 sec; Iteration 1241 / 4000) loss: 1.241340\n","(Time 7.23 sec; Iteration 1251 / 4000) loss: 1.228907\n","(Time 7.28 sec; Iteration 1261 / 4000) loss: 1.164295\n","(Time 7.33 sec; Iteration 1271 / 4000) loss: 1.211042\n","(Time 7.38 sec; Iteration 1281 / 4000) loss: 1.060709\n","(Time 7.43 sec; Iteration 1291 / 4000) loss: 1.165309\n","(Time 7.48 sec; Iteration 1301 / 4000) loss: 1.265169\n","(Time 7.53 sec; Iteration 1311 / 4000) loss: 1.082561\n","(Time 7.58 sec; Iteration 1321 / 4000) loss: 1.256138\n","(Time 7.63 sec; Iteration 1331 / 4000) loss: 1.117451\n","(Time 7.68 sec; Iteration 1341 / 4000) loss: 1.240093\n","(Time 7.73 sec; Iteration 1351 / 4000) loss: 1.115089\n","(Time 7.78 sec; Iteration 1361 / 4000) loss: 1.238129\n","(Time 7.84 sec; Iteration 1371 / 4000) loss: 1.023266\n","(Time 7.89 sec; Iteration 1381 / 4000) loss: 1.337340\n","(Time 7.94 sec; Iteration 1391 / 4000) loss: 1.145631\n","(Time 7.99 sec; Iteration 1401 / 4000) loss: 1.322393\n","(Time 8.04 sec; Iteration 1411 / 4000) loss: 1.149558\n","(Time 8.09 sec; Iteration 1421 / 4000) loss: 1.218332\n","(Time 8.14 sec; Iteration 1431 / 4000) loss: 1.222655\n","(Time 8.19 sec; Iteration 1441 / 4000) loss: 1.173660\n","(Time 8.24 sec; Iteration 1451 / 4000) loss: 1.129018\n","(Time 8.29 sec; Iteration 1461 / 4000) loss: 1.290592\n","(Time 8.34 sec; Iteration 1471 / 4000) loss: 1.120769\n","(Time 8.39 sec; Iteration 1481 / 4000) loss: 1.251153\n","(Time 8.44 sec; Iteration 1491 / 4000) loss: 1.056898\n","(Time 8.49 sec; Iteration 1501 / 4000) loss: 1.036835\n","(Time 8.54 sec; Iteration 1511 / 4000) loss: 1.195929\n","(Time 8.59 sec; Iteration 1521 / 4000) loss: 1.155723\n","(Time 8.64 sec; Iteration 1531 / 4000) loss: 1.141566\n","(Time 8.69 sec; Iteration 1541 / 4000) loss: 1.049324\n","(Time 8.74 sec; Iteration 1551 / 4000) loss: 1.202511\n","(Time 8.79 sec; Iteration 1561 / 4000) loss: 1.207912\n","(Time 8.84 sec; Iteration 1571 / 4000) loss: 1.239332\n","(Time 8.89 sec; Iteration 1581 / 4000) loss: 1.235398\n","(Time 8.94 sec; Iteration 1591 / 4000) loss: 1.209391\n","(Epoch 14 / 10) train acc: 0.610000; val_acc: 0.515900\n","(Time 9.16 sec; Iteration 1601 / 4000) loss: 1.189615\n","(Time 9.21 sec; Iteration 1611 / 4000) loss: 1.092001\n","(Time 9.26 sec; Iteration 1621 / 4000) loss: 1.118083\n","(Time 9.31 sec; Iteration 1631 / 4000) loss: 1.130206\n","(Time 9.36 sec; Iteration 1641 / 4000) loss: 1.165637\n","(Time 9.41 sec; Iteration 1651 / 4000) loss: 1.176309\n","(Time 9.46 sec; Iteration 1661 / 4000) loss: 0.898455\n","(Time 9.51 sec; Iteration 1671 / 4000) loss: 1.015665\n","(Time 9.56 sec; Iteration 1681 / 4000) loss: 1.266785\n","(Time 9.61 sec; Iteration 1691 / 4000) loss: 1.021955\n","(Time 9.66 sec; Iteration 1701 / 4000) loss: 1.057869\n","(Time 9.71 sec; Iteration 1711 / 4000) loss: 1.147909\n","(Time 9.76 sec; Iteration 1721 / 4000) loss: 1.122001\n","(Time 9.81 sec; Iteration 1731 / 4000) loss: 1.173859\n","(Time 9.86 sec; Iteration 1741 / 4000) loss: 1.177910\n","(Time 9.91 sec; Iteration 1751 / 4000) loss: 1.187378\n","(Time 9.96 sec; Iteration 1761 / 4000) loss: 1.188779\n","(Time 10.01 sec; Iteration 1771 / 4000) loss: 1.207242\n","(Time 10.06 sec; Iteration 1781 / 4000) loss: 1.060739\n","(Time 10.11 sec; Iteration 1791 / 4000) loss: 1.033242\n","(Time 10.16 sec; Iteration 1801 / 4000) loss: 1.210644\n","(Time 10.21 sec; Iteration 1811 / 4000) loss: 1.197988\n","(Time 10.26 sec; Iteration 1821 / 4000) loss: 0.988301\n","(Time 10.31 sec; Iteration 1831 / 4000) loss: 1.397546\n","(Time 10.36 sec; Iteration 1841 / 4000) loss: 1.278206\n","(Time 10.41 sec; Iteration 1851 / 4000) loss: 1.190200\n","(Time 10.46 sec; Iteration 1861 / 4000) loss: 1.065401\n","(Time 10.51 sec; Iteration 1871 / 4000) loss: 1.030364\n","(Time 10.56 sec; Iteration 1881 / 4000) loss: 1.187033\n","(Time 10.61 sec; Iteration 1891 / 4000) loss: 1.270723\n","(Time 10.66 sec; Iteration 1901 / 4000) loss: 1.334587\n","(Time 10.71 sec; Iteration 1911 / 4000) loss: 1.057129\n","(Time 10.76 sec; Iteration 1921 / 4000) loss: 1.207251\n","(Time 10.81 sec; Iteration 1931 / 4000) loss: 1.294233\n","(Time 10.86 sec; Iteration 1941 / 4000) loss: 1.202111\n","(Time 10.92 sec; Iteration 1951 / 4000) loss: 1.202677\n","(Time 10.97 sec; Iteration 1961 / 4000) loss: 1.273619\n","(Time 11.02 sec; Iteration 1971 / 4000) loss: 1.057764\n","(Time 11.07 sec; Iteration 1981 / 4000) loss: 1.187229\n","(Time 11.12 sec; Iteration 1991 / 4000) loss: 1.106436\n","(Epoch 15 / 10) train acc: 0.572000; val_acc: 0.514400\n","(Time 11.33 sec; Iteration 2001 / 4000) loss: 1.099981\n","(Time 11.38 sec; Iteration 2011 / 4000) loss: 1.182469\n","(Time 11.43 sec; Iteration 2021 / 4000) loss: 1.334012\n","(Time 11.48 sec; Iteration 2031 / 4000) loss: 1.111503\n","(Time 11.53 sec; Iteration 2041 / 4000) loss: 1.148136\n","(Time 11.58 sec; Iteration 2051 / 4000) loss: 1.248419\n","(Time 11.63 sec; Iteration 2061 / 4000) loss: 1.334178\n","(Time 11.68 sec; Iteration 2071 / 4000) loss: 1.024643\n","(Time 11.73 sec; Iteration 2081 / 4000) loss: 1.137704\n","(Time 11.78 sec; Iteration 2091 / 4000) loss: 1.136297\n","(Time 11.83 sec; Iteration 2101 / 4000) loss: 1.283294\n","(Time 11.88 sec; Iteration 2111 / 4000) loss: 1.204182\n","(Time 11.93 sec; Iteration 2121 / 4000) loss: 1.169661\n","(Time 11.98 sec; Iteration 2131 / 4000) loss: 1.290733\n","(Time 12.04 sec; Iteration 2141 / 4000) loss: 1.185162\n","(Time 12.09 sec; Iteration 2151 / 4000) loss: 1.057635\n","(Time 12.14 sec; Iteration 2161 / 4000) loss: 1.180193\n","(Time 12.19 sec; Iteration 2171 / 4000) loss: 1.235705\n","(Time 12.24 sec; Iteration 2181 / 4000) loss: 1.251829\n","(Time 12.29 sec; Iteration 2191 / 4000) loss: 1.197875\n","(Time 12.34 sec; Iteration 2201 / 4000) loss: 1.106642\n","(Time 12.39 sec; Iteration 2211 / 4000) loss: 1.244560\n","(Time 12.44 sec; Iteration 2221 / 4000) loss: 0.979840\n","(Time 12.49 sec; Iteration 2231 / 4000) loss: 1.292570\n","(Time 12.54 sec; Iteration 2241 / 4000) loss: 1.219383\n","(Time 12.59 sec; Iteration 2251 / 4000) loss: 1.191050\n","(Time 12.64 sec; Iteration 2261 / 4000) loss: 1.173317\n","(Time 12.69 sec; Iteration 2271 / 4000) loss: 1.017824\n","(Time 12.74 sec; Iteration 2281 / 4000) loss: 1.124062\n","(Time 12.79 sec; Iteration 2291 / 4000) loss: 1.183730\n","(Time 12.84 sec; Iteration 2301 / 4000) loss: 1.171745\n","(Time 12.89 sec; Iteration 2311 / 4000) loss: 1.139199\n","(Time 12.94 sec; Iteration 2321 / 4000) loss: 1.104539\n","(Time 13.00 sec; Iteration 2331 / 4000) loss: 1.078448\n","(Time 13.06 sec; Iteration 2341 / 4000) loss: 1.255367\n","(Time 13.12 sec; Iteration 2351 / 4000) loss: 1.107001\n","(Time 13.17 sec; Iteration 2361 / 4000) loss: 1.118890\n","(Time 13.23 sec; Iteration 2371 / 4000) loss: 1.176506\n","(Time 13.29 sec; Iteration 2381 / 4000) loss: 1.096894\n","(Time 13.34 sec; Iteration 2391 / 4000) loss: 1.240625\n","(Epoch 16 / 10) train acc: 0.610000; val_acc: 0.520800\n","(Time 13.56 sec; Iteration 2401 / 4000) loss: 1.258366\n","(Time 13.62 sec; Iteration 2411 / 4000) loss: 1.219118\n","(Time 13.67 sec; Iteration 2421 / 4000) loss: 1.118653\n","(Time 13.73 sec; Iteration 2431 / 4000) loss: 1.149825\n","(Time 13.79 sec; Iteration 2441 / 4000) loss: 1.275073\n","(Time 13.84 sec; Iteration 2451 / 4000) loss: 1.034835\n","(Time 13.90 sec; Iteration 2461 / 4000) loss: 1.238966\n","(Time 13.96 sec; Iteration 2471 / 4000) loss: 1.138465\n","(Time 14.02 sec; Iteration 2481 / 4000) loss: 0.948865\n","(Time 14.07 sec; Iteration 2491 / 4000) loss: 1.128714\n","(Time 14.13 sec; Iteration 2501 / 4000) loss: 0.935931\n","(Time 14.19 sec; Iteration 2511 / 4000) loss: 1.343721\n","(Time 14.25 sec; Iteration 2521 / 4000) loss: 1.146742\n","(Time 14.30 sec; Iteration 2531 / 4000) loss: 1.017642\n","(Time 14.36 sec; Iteration 2541 / 4000) loss: 1.161406\n","(Time 14.41 sec; Iteration 2551 / 4000) loss: 1.170129\n","(Time 14.47 sec; Iteration 2561 / 4000) loss: 1.128267\n","(Time 14.53 sec; Iteration 2571 / 4000) loss: 1.166936\n","(Time 14.58 sec; Iteration 2581 / 4000) loss: 1.170234\n","(Time 14.64 sec; Iteration 2591 / 4000) loss: 1.172014\n","(Time 14.70 sec; Iteration 2601 / 4000) loss: 1.040251\n","(Time 14.76 sec; Iteration 2611 / 4000) loss: 1.238092\n","(Time 14.81 sec; Iteration 2621 / 4000) loss: 1.291621\n","(Time 14.86 sec; Iteration 2631 / 4000) loss: 1.333935\n","(Time 14.92 sec; Iteration 2641 / 4000) loss: 1.144787\n","(Time 14.98 sec; Iteration 2651 / 4000) loss: 1.087329\n","(Time 15.04 sec; Iteration 2661 / 4000) loss: 1.019033\n","(Time 15.09 sec; Iteration 2671 / 4000) loss: 1.163000\n","(Time 15.15 sec; Iteration 2681 / 4000) loss: 1.260691\n","(Time 15.21 sec; Iteration 2691 / 4000) loss: 1.121659\n","(Time 15.27 sec; Iteration 2701 / 4000) loss: 1.122291\n","(Time 15.32 sec; Iteration 2711 / 4000) loss: 1.122997\n","(Time 15.39 sec; Iteration 2721 / 4000) loss: 1.307585\n","(Time 15.44 sec; Iteration 2731 / 4000) loss: 1.286618\n","(Time 15.50 sec; Iteration 2741 / 4000) loss: 1.241520\n","(Time 15.55 sec; Iteration 2751 / 4000) loss: 1.312942\n","(Time 15.61 sec; Iteration 2761 / 4000) loss: 1.033852\n","(Time 15.66 sec; Iteration 2771 / 4000) loss: 1.176565\n","(Time 15.72 sec; Iteration 2781 / 4000) loss: 1.260543\n","(Time 15.77 sec; Iteration 2791 / 4000) loss: 1.280319\n","(Epoch 17 / 10) train acc: 0.631000; val_acc: 0.518700\n","(Time 15.99 sec; Iteration 2801 / 4000) loss: 1.172857\n","(Time 16.05 sec; Iteration 2811 / 4000) loss: 1.242860\n","(Time 16.11 sec; Iteration 2821 / 4000) loss: 1.092703\n","(Time 16.16 sec; Iteration 2831 / 4000) loss: 1.055147\n","(Time 16.22 sec; Iteration 2841 / 4000) loss: 1.283805\n","(Time 16.28 sec; Iteration 2851 / 4000) loss: 1.034392\n","(Time 16.34 sec; Iteration 2861 / 4000) loss: 1.050565\n","(Time 16.39 sec; Iteration 2871 / 4000) loss: 1.103541\n","(Time 16.44 sec; Iteration 2881 / 4000) loss: 1.290675\n","(Time 16.49 sec; Iteration 2891 / 4000) loss: 1.155425\n","(Time 16.55 sec; Iteration 2901 / 4000) loss: 1.154122\n","(Time 16.59 sec; Iteration 2911 / 4000) loss: 1.198769\n","(Time 16.64 sec; Iteration 2921 / 4000) loss: 1.057571\n","(Time 16.69 sec; Iteration 2931 / 4000) loss: 1.123231\n","(Time 16.74 sec; Iteration 2941 / 4000) loss: 1.034813\n","(Time 16.79 sec; Iteration 2951 / 4000) loss: 1.196746\n","(Time 16.84 sec; Iteration 2961 / 4000) loss: 1.187957\n","(Time 16.90 sec; Iteration 2971 / 4000) loss: 1.078882\n","(Time 16.95 sec; Iteration 2981 / 4000) loss: 1.167054\n","(Time 17.00 sec; Iteration 2991 / 4000) loss: 1.125648\n","(Time 17.05 sec; Iteration 3001 / 4000) loss: 1.069738\n","(Time 17.10 sec; Iteration 3011 / 4000) loss: 1.175473\n","(Time 17.15 sec; Iteration 3021 / 4000) loss: 1.122921\n","(Time 17.20 sec; Iteration 3031 / 4000) loss: 1.067835\n","(Time 17.25 sec; Iteration 3041 / 4000) loss: 1.127975\n","(Time 17.30 sec; Iteration 3051 / 4000) loss: 0.960838\n","(Time 17.35 sec; Iteration 3061 / 4000) loss: 1.260987\n","(Time 17.40 sec; Iteration 3071 / 4000) loss: 0.999111\n","(Time 17.45 sec; Iteration 3081 / 4000) loss: 1.163311\n","(Time 17.50 sec; Iteration 3091 / 4000) loss: 1.100783\n","(Time 17.55 sec; Iteration 3101 / 4000) loss: 1.140463\n","(Time 17.60 sec; Iteration 3111 / 4000) loss: 0.976461\n","(Time 17.65 sec; Iteration 3121 / 4000) loss: 1.122804\n","(Time 17.70 sec; Iteration 3131 / 4000) loss: 1.280548\n","(Time 17.75 sec; Iteration 3141 / 4000) loss: 1.286371\n","(Time 17.80 sec; Iteration 3151 / 4000) loss: 1.193964\n","(Time 17.85 sec; Iteration 3161 / 4000) loss: 1.138001\n","(Time 17.90 sec; Iteration 3171 / 4000) loss: 1.310421\n","(Time 17.95 sec; Iteration 3181 / 4000) loss: 1.253979\n","(Time 18.00 sec; Iteration 3191 / 4000) loss: 1.070077\n","(Epoch 18 / 10) train acc: 0.589000; val_acc: 0.521900\n","(Time 18.21 sec; Iteration 3201 / 4000) loss: 0.968991\n","(Time 18.26 sec; Iteration 3211 / 4000) loss: 1.127822\n","(Time 18.31 sec; Iteration 3221 / 4000) loss: 1.163994\n","(Time 18.36 sec; Iteration 3231 / 4000) loss: 1.146568\n","(Time 18.41 sec; Iteration 3241 / 4000) loss: 1.132255\n","(Time 18.46 sec; Iteration 3251 / 4000) loss: 1.191139\n","(Time 18.51 sec; Iteration 3261 / 4000) loss: 1.075964\n","(Time 18.56 sec; Iteration 3271 / 4000) loss: 1.142708\n","(Time 18.61 sec; Iteration 3281 / 4000) loss: 0.988419\n","(Time 18.66 sec; Iteration 3291 / 4000) loss: 1.230333\n","(Time 18.71 sec; Iteration 3301 / 4000) loss: 1.214929\n","(Time 18.76 sec; Iteration 3311 / 4000) loss: 1.255305\n","(Time 18.81 sec; Iteration 3321 / 4000) loss: 1.382233\n","(Time 18.86 sec; Iteration 3331 / 4000) loss: 1.103908\n","(Time 18.91 sec; Iteration 3341 / 4000) loss: 1.243779\n","(Time 18.96 sec; Iteration 3351 / 4000) loss: 1.263448\n","(Time 19.01 sec; Iteration 3361 / 4000) loss: 1.038509\n","(Time 19.06 sec; Iteration 3371 / 4000) loss: 1.244285\n","(Time 19.11 sec; Iteration 3381 / 4000) loss: 1.168864\n","(Time 19.16 sec; Iteration 3391 / 4000) loss: 1.271744\n","(Time 19.21 sec; Iteration 3401 / 4000) loss: 1.097516\n","(Time 19.26 sec; Iteration 3411 / 4000) loss: 1.242232\n","(Time 19.31 sec; Iteration 3421 / 4000) loss: 1.101372\n","(Time 19.36 sec; Iteration 3431 / 4000) loss: 1.298714\n","(Time 19.41 sec; Iteration 3441 / 4000) loss: 1.152992\n","(Time 19.46 sec; Iteration 3451 / 4000) loss: 1.123202\n","(Time 19.51 sec; Iteration 3461 / 4000) loss: 1.366706\n","(Time 19.56 sec; Iteration 3471 / 4000) loss: 1.001948\n","(Time 19.61 sec; Iteration 3481 / 4000) loss: 1.185009\n","(Time 19.66 sec; Iteration 3491 / 4000) loss: 1.288454\n","(Time 19.71 sec; Iteration 3501 / 4000) loss: 1.019447\n","(Time 19.76 sec; Iteration 3511 / 4000) loss: 0.982181\n","(Time 19.81 sec; Iteration 3521 / 4000) loss: 1.332543\n","(Time 19.86 sec; Iteration 3531 / 4000) loss: 1.020809\n","(Time 19.91 sec; Iteration 3541 / 4000) loss: 1.223704\n","(Time 19.96 sec; Iteration 3551 / 4000) loss: 1.112859\n","(Time 20.01 sec; Iteration 3561 / 4000) loss: 1.166720\n","(Time 20.06 sec; Iteration 3571 / 4000) loss: 1.036397\n","(Time 20.11 sec; Iteration 3581 / 4000) loss: 1.318474\n","(Time 20.16 sec; Iteration 3591 / 4000) loss: 1.078433\n","(Epoch 19 / 10) train acc: 0.620000; val_acc: 0.523000\n","(Time 20.38 sec; Iteration 3601 / 4000) loss: 1.290784\n","(Time 20.43 sec; Iteration 3611 / 4000) loss: 1.219504\n","(Time 20.48 sec; Iteration 3621 / 4000) loss: 1.146533\n","(Time 20.52 sec; Iteration 3631 / 4000) loss: 1.240276\n","(Time 20.57 sec; Iteration 3641 / 4000) loss: 1.159964\n","(Time 20.63 sec; Iteration 3651 / 4000) loss: 1.044392\n","(Time 20.68 sec; Iteration 3661 / 4000) loss: 1.219519\n","(Time 20.72 sec; Iteration 3671 / 4000) loss: 1.102509\n","(Time 20.78 sec; Iteration 3681 / 4000) loss: 1.041080\n","(Time 20.83 sec; Iteration 3691 / 4000) loss: 1.292995\n","(Time 20.88 sec; Iteration 3701 / 4000) loss: 1.106796\n","(Time 20.93 sec; Iteration 3711 / 4000) loss: 1.113122\n","(Time 20.98 sec; Iteration 3721 / 4000) loss: 1.078834\n","(Time 21.03 sec; Iteration 3731 / 4000) loss: 1.128816\n","(Time 21.08 sec; Iteration 3741 / 4000) loss: 1.206540\n","(Time 21.13 sec; Iteration 3751 / 4000) loss: 1.040317\n","(Time 21.18 sec; Iteration 3761 / 4000) loss: 1.138518\n","(Time 21.23 sec; Iteration 3771 / 4000) loss: 1.175256\n","(Time 21.28 sec; Iteration 3781 / 4000) loss: 1.140854\n","(Time 21.33 sec; Iteration 3791 / 4000) loss: 1.243422\n","(Time 21.38 sec; Iteration 3801 / 4000) loss: 1.048982\n","(Time 21.43 sec; Iteration 3811 / 4000) loss: 1.214869\n","(Time 21.48 sec; Iteration 3821 / 4000) loss: 1.178481\n","(Time 21.53 sec; Iteration 3831 / 4000) loss: 1.093995\n","(Time 21.58 sec; Iteration 3841 / 4000) loss: 1.080154\n","(Time 21.63 sec; Iteration 3851 / 4000) loss: 0.976751\n","(Time 21.68 sec; Iteration 3861 / 4000) loss: 1.146700\n","(Time 21.73 sec; Iteration 3871 / 4000) loss: 0.958840\n","(Time 21.78 sec; Iteration 3881 / 4000) loss: 1.204129\n","(Time 21.83 sec; Iteration 3891 / 4000) loss: 1.214586\n","(Time 21.88 sec; Iteration 3901 / 4000) loss: 1.180880\n","(Time 21.93 sec; Iteration 3911 / 4000) loss: 1.135145\n","(Time 21.98 sec; Iteration 3921 / 4000) loss: 1.253836\n","(Time 22.03 sec; Iteration 3931 / 4000) loss: 1.152633\n","(Time 22.08 sec; Iteration 3941 / 4000) loss: 1.248403\n","(Time 22.13 sec; Iteration 3951 / 4000) loss: 1.201165\n","(Time 22.19 sec; Iteration 3961 / 4000) loss: 1.102940\n","(Time 22.24 sec; Iteration 3971 / 4000) loss: 1.162844\n","(Time 22.29 sec; Iteration 3981 / 4000) loss: 1.060152\n","(Time 22.34 sec; Iteration 3991 / 4000) loss: 1.034627\n","(Epoch 20 / 10) train acc: 0.618000; val_acc: 0.518700\n"]}],"source":["from fully_connected_networks import create_solver_instance\n","\n","reset_seed(0)\n","# Create a solver instance that achieves 50% performance on the validation set\n","solver = create_solver_instance(data_dict=data_dict, dtype=torch.float64, device='cuda')\n","solver.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"gSSy7LTde1DE","new_sheet":false,"run_control":{"read_only":false},"colab":{"base_uri":"https://localhost:8080/","height":745},"executionInfo":{"status":"ok","timestamp":1677955269300,"user_tz":-180,"elapsed":732,"user":{"displayName":"Muhammet Can Gümüşsu","userId":"11484611351929850578"}},"outputId":"21dcd498-429a-45a0-9a6e-d0945223cd27"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x864 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["# Run this cell to visualize training loss and train / val accuracy\n","plt.subplot(2, 1, 1)\n","plt.title('Training loss')\n","plt.plot(solver.loss_history, 'o')\n","plt.xlabel('Iteration')\n","\n","plt.subplot(2, 1, 2)\n","plt.title('Accuracy')\n","plt.plot(solver.train_acc_history, '-o', label='train')\n","plt.plot(solver.val_acc_history, '-o', label='val')\n","plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n","plt.xlabel('Epoch')\n","plt.legend(loc='lower right')\n","plt.gcf().set_size_inches(15, 12)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"oUwvMomE31Mh","new_sheet":false,"run_control":{"read_only":false}},"source":["If you're happy with the model's perfromance, run the following cell to save it. \n","\n","We will also reload the model and run it on validation to verify it's the right weights."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"AfE_2VVK31fa","new_sheet":false,"run_control":{"read_only":false},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677955349883,"user_tz":-180,"elapsed":23144,"user":{"displayName":"Muhammet Can Gümüşsu","userId":"11484611351929850578"}},"outputId":"2ef8a72f-2fa1-43b5-f006-68ce60250930"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saved in /content/drive/MyDrive/Mich.ComputerVisionAssignment1/A3/best_two_layer_net.pth\n","(Time 0.01 sec; Iteration 1 / 4000) loss: 2.302603\n","(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.092200\n","(Time 0.39 sec; Iteration 11 / 4000) loss: 2.302131\n","(Time 0.44 sec; Iteration 21 / 4000) loss: 2.301974\n","(Time 0.49 sec; Iteration 31 / 4000) loss: 2.302352\n","(Time 0.54 sec; Iteration 41 / 4000) loss: 2.297661\n","(Time 0.60 sec; Iteration 51 / 4000) loss: 2.283626\n","(Time 0.65 sec; Iteration 61 / 4000) loss: 2.268614\n","(Time 0.70 sec; Iteration 71 / 4000) loss: 2.212975\n","(Time 0.75 sec; Iteration 81 / 4000) loss: 2.185241\n","(Time 0.80 sec; Iteration 91 / 4000) loss: 2.173307\n","(Time 0.85 sec; Iteration 101 / 4000) loss: 2.183683\n","(Time 0.90 sec; Iteration 111 / 4000) loss: 2.091396\n","(Time 0.95 sec; Iteration 121 / 4000) loss: 2.059497\n","(Time 1.00 sec; Iteration 131 / 4000) loss: 2.073542\n","(Time 1.05 sec; Iteration 141 / 4000) loss: 2.043060\n","(Time 1.10 sec; Iteration 151 / 4000) loss: 2.057305\n","(Time 1.15 sec; Iteration 161 / 4000) loss: 2.060444\n","(Time 1.20 sec; Iteration 171 / 4000) loss: 1.969404\n","(Time 1.25 sec; Iteration 181 / 4000) loss: 2.018749\n","(Time 1.30 sec; Iteration 191 / 4000) loss: 1.918732\n","(Time 1.35 sec; Iteration 201 / 4000) loss: 1.925371\n","(Time 1.40 sec; Iteration 211 / 4000) loss: 1.986776\n","(Time 1.45 sec; Iteration 221 / 4000) loss: 2.040569\n","(Time 1.50 sec; Iteration 231 / 4000) loss: 1.862027\n","(Time 1.55 sec; Iteration 241 / 4000) loss: 1.882783\n","(Time 1.60 sec; Iteration 251 / 4000) loss: 1.999209\n","(Time 1.65 sec; Iteration 261 / 4000) loss: 1.881364\n","(Time 1.70 sec; Iteration 271 / 4000) loss: 1.869433\n","(Time 1.75 sec; Iteration 281 / 4000) loss: 1.920360\n","(Time 1.80 sec; Iteration 291 / 4000) loss: 1.734718\n","(Time 1.85 sec; Iteration 301 / 4000) loss: 1.746224\n","(Time 1.90 sec; Iteration 311 / 4000) loss: 1.827956\n","(Time 1.95 sec; Iteration 321 / 4000) loss: 1.936580\n","(Time 2.00 sec; Iteration 331 / 4000) loss: 1.991148\n","(Time 2.05 sec; Iteration 341 / 4000) loss: 1.981817\n","(Time 2.10 sec; Iteration 351 / 4000) loss: 1.872227\n","(Time 2.15 sec; Iteration 361 / 4000) loss: 1.688998\n","(Time 2.20 sec; Iteration 371 / 4000) loss: 1.839584\n","(Time 2.25 sec; Iteration 381 / 4000) loss: 1.810971\n","(Time 2.30 sec; Iteration 391 / 4000) loss: 1.808983\n","(Epoch 1 / 10) train acc: 0.361000; val_acc: 0.359000\n","(Time 2.51 sec; Iteration 401 / 4000) loss: 1.866122\n","(Time 2.57 sec; Iteration 411 / 4000) loss: 1.905465\n","(Time 2.62 sec; Iteration 421 / 4000) loss: 1.730916\n","(Time 2.67 sec; Iteration 431 / 4000) loss: 1.752849\n","(Time 2.72 sec; Iteration 441 / 4000) loss: 1.692051\n","(Time 2.77 sec; Iteration 451 / 4000) loss: 1.627449\n","(Time 2.82 sec; Iteration 461 / 4000) loss: 1.766686\n","(Time 2.87 sec; Iteration 471 / 4000) loss: 1.754263\n","(Time 2.92 sec; Iteration 481 / 4000) loss: 1.680517\n","(Time 2.97 sec; Iteration 491 / 4000) loss: 1.819147\n","(Time 3.02 sec; Iteration 501 / 4000) loss: 1.848964\n","(Time 3.07 sec; Iteration 511 / 4000) loss: 1.664147\n","(Time 3.12 sec; Iteration 521 / 4000) loss: 1.861571\n","(Time 3.17 sec; Iteration 531 / 4000) loss: 1.566806\n","(Time 3.22 sec; Iteration 541 / 4000) loss: 1.747006\n","(Time 3.27 sec; Iteration 551 / 4000) loss: 1.634745\n","(Time 3.32 sec; Iteration 561 / 4000) loss: 1.684015\n","(Time 3.37 sec; Iteration 571 / 4000) loss: 1.847447\n","(Time 3.42 sec; Iteration 581 / 4000) loss: 1.796701\n","(Time 3.47 sec; Iteration 591 / 4000) loss: 1.698253\n","(Time 3.52 sec; Iteration 601 / 4000) loss: 1.626178\n","(Time 3.57 sec; Iteration 611 / 4000) loss: 1.793206\n","(Time 3.62 sec; Iteration 621 / 4000) loss: 1.764089\n","(Time 3.68 sec; Iteration 631 / 4000) loss: 1.776800\n","(Time 3.73 sec; Iteration 641 / 4000) loss: 1.736117\n","(Time 3.78 sec; Iteration 651 / 4000) loss: 1.686904\n","(Time 3.83 sec; Iteration 661 / 4000) loss: 1.697896\n","(Time 3.88 sec; Iteration 671 / 4000) loss: 1.786791\n","(Time 3.93 sec; Iteration 681 / 4000) loss: 1.563450\n","(Time 3.98 sec; Iteration 691 / 4000) loss: 1.567511\n","(Time 4.03 sec; Iteration 701 / 4000) loss: 1.760979\n","(Time 4.08 sec; Iteration 711 / 4000) loss: 1.665903\n","(Time 4.13 sec; Iteration 721 / 4000) loss: 1.715495\n","(Time 4.18 sec; Iteration 731 / 4000) loss: 1.478780\n","(Time 4.23 sec; Iteration 741 / 4000) loss: 1.593218\n","(Time 4.28 sec; Iteration 751 / 4000) loss: 1.464391\n","(Time 4.33 sec; Iteration 761 / 4000) loss: 1.576131\n","(Time 4.38 sec; Iteration 771 / 4000) loss: 1.585751\n","(Time 4.43 sec; Iteration 781 / 4000) loss: 1.493866\n","(Time 4.48 sec; Iteration 791 / 4000) loss: 1.659011\n","(Epoch 2 / 10) train acc: 0.429000; val_acc: 0.416400\n","(Time 4.69 sec; Iteration 801 / 4000) loss: 1.522012\n","(Time 4.74 sec; Iteration 811 / 4000) loss: 1.524754\n","(Time 4.79 sec; Iteration 821 / 4000) loss: 1.693718\n","(Time 4.84 sec; Iteration 831 / 4000) loss: 1.619641\n","(Time 4.89 sec; Iteration 841 / 4000) loss: 1.661180\n","(Time 4.94 sec; Iteration 851 / 4000) loss: 1.731999\n","(Time 4.99 sec; Iteration 861 / 4000) loss: 1.388202\n","(Time 5.05 sec; Iteration 871 / 4000) loss: 1.494590\n","(Time 5.10 sec; Iteration 881 / 4000) loss: 1.719062\n","(Time 5.15 sec; Iteration 891 / 4000) loss: 1.696230\n","(Time 5.20 sec; Iteration 901 / 4000) loss: 1.720399\n","(Time 5.25 sec; Iteration 911 / 4000) loss: 1.634952\n","(Time 5.30 sec; Iteration 921 / 4000) loss: 1.641377\n","(Time 5.35 sec; Iteration 931 / 4000) loss: 1.613847\n","(Time 5.40 sec; Iteration 941 / 4000) loss: 1.416605\n","(Time 5.45 sec; Iteration 951 / 4000) loss: 1.493810\n","(Time 5.50 sec; Iteration 961 / 4000) loss: 1.609787\n","(Time 5.54 sec; Iteration 971 / 4000) loss: 1.434485\n","(Time 5.60 sec; Iteration 981 / 4000) loss: 1.673444\n","(Time 5.65 sec; Iteration 991 / 4000) loss: 1.492794\n","(Time 5.70 sec; Iteration 1001 / 4000) loss: 1.417730\n","(Time 5.75 sec; Iteration 1011 / 4000) loss: 1.515685\n","(Time 5.80 sec; Iteration 1021 / 4000) loss: 1.690853\n","(Time 5.85 sec; Iteration 1031 / 4000) loss: 1.463434\n","(Time 5.90 sec; Iteration 1041 / 4000) loss: 1.534649\n","(Time 5.95 sec; Iteration 1051 / 4000) loss: 1.518368\n","(Time 6.00 sec; Iteration 1061 / 4000) loss: 1.572649\n","(Time 6.05 sec; Iteration 1071 / 4000) loss: 1.496677\n","(Time 6.10 sec; Iteration 1081 / 4000) loss: 1.467277\n","(Time 6.15 sec; Iteration 1091 / 4000) loss: 1.411377\n","(Time 6.20 sec; Iteration 1101 / 4000) loss: 1.508326\n","(Time 6.25 sec; Iteration 1111 / 4000) loss: 1.565717\n","(Time 6.30 sec; Iteration 1121 / 4000) loss: 1.502134\n","(Time 6.35 sec; Iteration 1131 / 4000) loss: 1.524817\n","(Time 6.40 sec; Iteration 1141 / 4000) loss: 1.462300\n","(Time 6.45 sec; Iteration 1151 / 4000) loss: 1.596607\n","(Time 6.50 sec; Iteration 1161 / 4000) loss: 1.412042\n","(Time 6.55 sec; Iteration 1171 / 4000) loss: 1.525309\n","(Time 6.60 sec; Iteration 1181 / 4000) loss: 1.389256\n","(Time 6.65 sec; Iteration 1191 / 4000) loss: 1.458935\n","(Epoch 3 / 10) train acc: 0.447000; val_acc: 0.441400\n","(Time 6.86 sec; Iteration 1201 / 4000) loss: 1.573176\n","(Time 6.91 sec; Iteration 1211 / 4000) loss: 1.467197\n","(Time 6.96 sec; Iteration 1221 / 4000) loss: 1.607835\n","(Time 7.01 sec; Iteration 1231 / 4000) loss: 1.537023\n","(Time 7.06 sec; Iteration 1241 / 4000) loss: 1.591737\n","(Time 7.11 sec; Iteration 1251 / 4000) loss: 1.525906\n","(Time 7.16 sec; Iteration 1261 / 4000) loss: 1.488748\n","(Time 7.21 sec; Iteration 1271 / 4000) loss: 1.342857\n","(Time 7.26 sec; Iteration 1281 / 4000) loss: 1.487766\n","(Time 7.32 sec; Iteration 1291 / 4000) loss: 1.296228\n","(Time 7.37 sec; Iteration 1301 / 4000) loss: 1.415928\n","(Time 7.42 sec; Iteration 1311 / 4000) loss: 1.487382\n","(Time 7.47 sec; Iteration 1321 / 4000) loss: 1.467810\n","(Time 7.52 sec; Iteration 1331 / 4000) loss: 1.560782\n","(Time 7.57 sec; Iteration 1341 / 4000) loss: 1.538561\n","(Time 7.62 sec; Iteration 1351 / 4000) loss: 1.659575\n","(Time 7.67 sec; Iteration 1361 / 4000) loss: 1.516713\n","(Time 7.72 sec; Iteration 1371 / 4000) loss: 1.435643\n","(Time 7.77 sec; Iteration 1381 / 4000) loss: 1.490255\n","(Time 7.82 sec; Iteration 1391 / 4000) loss: 1.615745\n","(Time 7.87 sec; Iteration 1401 / 4000) loss: 1.681612\n","(Time 7.92 sec; Iteration 1411 / 4000) loss: 1.311217\n","(Time 7.97 sec; Iteration 1421 / 4000) loss: 1.531865\n","(Time 8.03 sec; Iteration 1431 / 4000) loss: 1.448577\n","(Time 8.08 sec; Iteration 1441 / 4000) loss: 1.437073\n","(Time 8.12 sec; Iteration 1451 / 4000) loss: 1.616219\n","(Time 8.18 sec; Iteration 1461 / 4000) loss: 1.583338\n","(Time 8.23 sec; Iteration 1471 / 4000) loss: 1.532556\n","(Time 8.28 sec; Iteration 1481 / 4000) loss: 1.563625\n","(Time 8.33 sec; Iteration 1491 / 4000) loss: 1.614664\n","(Time 8.38 sec; Iteration 1501 / 4000) loss: 1.386953\n","(Time 8.43 sec; Iteration 1511 / 4000) loss: 1.421970\n","(Time 8.48 sec; Iteration 1521 / 4000) loss: 1.266908\n","(Time 8.53 sec; Iteration 1531 / 4000) loss: 1.407091\n","(Time 8.58 sec; Iteration 1541 / 4000) loss: 1.641110\n","(Time 8.63 sec; Iteration 1551 / 4000) loss: 1.633930\n","(Time 8.68 sec; Iteration 1561 / 4000) loss: 1.402136\n","(Time 8.73 sec; Iteration 1571 / 4000) loss: 1.354367\n","(Time 8.78 sec; Iteration 1581 / 4000) loss: 1.241307\n","(Time 8.83 sec; Iteration 1591 / 4000) loss: 1.481929\n","(Epoch 4 / 10) train acc: 0.463000; val_acc: 0.459100\n","(Time 9.04 sec; Iteration 1601 / 4000) loss: 1.521766\n","(Time 9.09 sec; Iteration 1611 / 4000) loss: 1.242913\n","(Time 9.14 sec; Iteration 1621 / 4000) loss: 1.367333\n","(Time 9.19 sec; Iteration 1631 / 4000) loss: 1.551271\n","(Time 9.25 sec; Iteration 1641 / 4000) loss: 1.547451\n","(Time 9.30 sec; Iteration 1651 / 4000) loss: 1.404027\n","(Time 9.35 sec; Iteration 1661 / 4000) loss: 1.451265\n","(Time 9.40 sec; Iteration 1671 / 4000) loss: 1.512043\n","(Time 9.45 sec; Iteration 1681 / 4000) loss: 1.411674\n","(Time 9.50 sec; Iteration 1691 / 4000) loss: 1.390853\n","(Time 9.55 sec; Iteration 1701 / 4000) loss: 1.335102\n","(Time 9.60 sec; Iteration 1711 / 4000) loss: 1.486694\n","(Time 9.65 sec; Iteration 1721 / 4000) loss: 1.555205\n","(Time 9.70 sec; Iteration 1731 / 4000) loss: 1.563746\n","(Time 9.75 sec; Iteration 1741 / 4000) loss: 1.477284\n","(Time 9.80 sec; Iteration 1751 / 4000) loss: 1.505864\n","(Time 9.85 sec; Iteration 1761 / 4000) loss: 1.271154\n","(Time 9.90 sec; Iteration 1771 / 4000) loss: 1.525193\n","(Time 9.95 sec; Iteration 1781 / 4000) loss: 1.389323\n","(Time 10.00 sec; Iteration 1791 / 4000) loss: 1.468372\n","(Time 10.05 sec; Iteration 1801 / 4000) loss: 1.417168\n","(Time 10.10 sec; Iteration 1811 / 4000) loss: 1.416766\n","(Time 10.16 sec; Iteration 1821 / 4000) loss: 1.508289\n","(Time 10.22 sec; Iteration 1831 / 4000) loss: 1.426444\n","(Time 10.27 sec; Iteration 1841 / 4000) loss: 1.395155\n","(Time 10.33 sec; Iteration 1851 / 4000) loss: 1.336724\n","(Time 10.39 sec; Iteration 1861 / 4000) loss: 1.288793\n","(Time 10.45 sec; Iteration 1871 / 4000) loss: 1.305221\n","(Time 10.50 sec; Iteration 1881 / 4000) loss: 1.337113\n","(Time 10.56 sec; Iteration 1891 / 4000) loss: 1.498912\n","(Time 10.61 sec; Iteration 1901 / 4000) loss: 1.364826\n","(Time 10.67 sec; Iteration 1911 / 4000) loss: 1.266581\n","(Time 10.72 sec; Iteration 1921 / 4000) loss: 1.352975\n","(Time 10.78 sec; Iteration 1931 / 4000) loss: 1.417407\n","(Time 10.84 sec; Iteration 1941 / 4000) loss: 1.237848\n","(Time 10.90 sec; Iteration 1951 / 4000) loss: 1.352375\n","(Time 10.95 sec; Iteration 1961 / 4000) loss: 1.264502\n","(Time 11.01 sec; Iteration 1971 / 4000) loss: 1.476294\n","(Time 11.06 sec; Iteration 1981 / 4000) loss: 1.172755\n","(Time 11.12 sec; Iteration 1991 / 4000) loss: 1.470435\n","(Epoch 5 / 10) train acc: 0.517000; val_acc: 0.474700\n","(Time 11.34 sec; Iteration 2001 / 4000) loss: 1.412173\n","(Time 11.39 sec; Iteration 2011 / 4000) loss: 1.365631\n","(Time 11.45 sec; Iteration 2021 / 4000) loss: 1.380066\n","(Time 11.50 sec; Iteration 2031 / 4000) loss: 1.388442\n","(Time 11.56 sec; Iteration 2041 / 4000) loss: 1.406061\n","(Time 11.62 sec; Iteration 2051 / 4000) loss: 1.527548\n","(Time 11.68 sec; Iteration 2061 / 4000) loss: 1.423839\n","(Time 11.73 sec; Iteration 2071 / 4000) loss: 1.351541\n","(Time 11.79 sec; Iteration 2081 / 4000) loss: 1.392506\n","(Time 11.85 sec; Iteration 2091 / 4000) loss: 1.415820\n","(Time 11.91 sec; Iteration 2101 / 4000) loss: 1.368111\n","(Time 11.96 sec; Iteration 2111 / 4000) loss: 1.495955\n","(Time 12.02 sec; Iteration 2121 / 4000) loss: 1.439826\n","(Time 12.07 sec; Iteration 2131 / 4000) loss: 1.424299\n","(Time 12.13 sec; Iteration 2141 / 4000) loss: 1.480144\n","(Time 12.18 sec; Iteration 2151 / 4000) loss: 1.535295\n","(Time 12.24 sec; Iteration 2161 / 4000) loss: 1.348136\n","(Time 12.30 sec; Iteration 2171 / 4000) loss: 1.192357\n","(Time 12.36 sec; Iteration 2181 / 4000) loss: 1.474535\n","(Time 12.41 sec; Iteration 2191 / 4000) loss: 1.315101\n","(Time 12.47 sec; Iteration 2201 / 4000) loss: 1.386365\n","(Time 12.53 sec; Iteration 2211 / 4000) loss: 1.292390\n","(Time 12.58 sec; Iteration 2221 / 4000) loss: 1.374970\n","(Time 12.63 sec; Iteration 2231 / 4000) loss: 1.224959\n","(Time 12.69 sec; Iteration 2241 / 4000) loss: 1.395312\n","(Time 12.76 sec; Iteration 2251 / 4000) loss: 1.419005\n","(Time 12.81 sec; Iteration 2261 / 4000) loss: 1.312549\n","(Time 12.87 sec; Iteration 2271 / 4000) loss: 1.373220\n","(Time 12.93 sec; Iteration 2281 / 4000) loss: 1.416466\n","(Time 12.98 sec; Iteration 2291 / 4000) loss: 1.237260\n","(Time 13.04 sec; Iteration 2301 / 4000) loss: 1.500228\n","(Time 13.09 sec; Iteration 2311 / 4000) loss: 1.371111\n","(Time 13.15 sec; Iteration 2321 / 4000) loss: 1.398013\n","(Time 13.21 sec; Iteration 2331 / 4000) loss: 1.454128\n","(Time 13.26 sec; Iteration 2341 / 4000) loss: 1.293662\n","(Time 13.32 sec; Iteration 2351 / 4000) loss: 1.318695\n","(Time 13.38 sec; Iteration 2361 / 4000) loss: 1.328741\n","(Time 13.43 sec; Iteration 2371 / 4000) loss: 1.452432\n","(Time 13.49 sec; Iteration 2381 / 4000) loss: 1.607329\n","(Time 13.54 sec; Iteration 2391 / 4000) loss: 1.353413\n","(Epoch 6 / 10) train acc: 0.549000; val_acc: 0.486500\n","(Time 13.75 sec; Iteration 2401 / 4000) loss: 1.341348\n","(Time 13.80 sec; Iteration 2411 / 4000) loss: 1.454511\n","(Time 13.85 sec; Iteration 2421 / 4000) loss: 1.530261\n","(Time 13.90 sec; Iteration 2431 / 4000) loss: 1.269506\n","(Time 13.96 sec; Iteration 2441 / 4000) loss: 1.215866\n","(Time 14.01 sec; Iteration 2451 / 4000) loss: 1.234394\n","(Time 14.06 sec; Iteration 2461 / 4000) loss: 1.497389\n","(Time 14.11 sec; Iteration 2471 / 4000) loss: 1.437315\n","(Time 14.15 sec; Iteration 2481 / 4000) loss: 1.112067\n","(Time 14.20 sec; Iteration 2491 / 4000) loss: 1.571989\n","(Time 14.25 sec; Iteration 2501 / 4000) loss: 1.347804\n","(Time 14.31 sec; Iteration 2511 / 4000) loss: 1.473773\n","(Time 14.36 sec; Iteration 2521 / 4000) loss: 1.311973\n","(Time 14.41 sec; Iteration 2531 / 4000) loss: 1.300632\n","(Time 14.46 sec; Iteration 2541 / 4000) loss: 1.323950\n","(Time 14.51 sec; Iteration 2551 / 4000) loss: 1.197390\n","(Time 14.56 sec; Iteration 2561 / 4000) loss: 1.297808\n","(Time 14.60 sec; Iteration 2571 / 4000) loss: 1.443908\n","(Time 14.66 sec; Iteration 2581 / 4000) loss: 1.362915\n","(Time 14.71 sec; Iteration 2591 / 4000) loss: 1.387469\n","(Time 14.76 sec; Iteration 2601 / 4000) loss: 1.197494\n","(Time 14.81 sec; Iteration 2611 / 4000) loss: 1.331915\n","(Time 14.86 sec; Iteration 2621 / 4000) loss: 1.407377\n","(Time 14.91 sec; Iteration 2631 / 4000) loss: 1.348649\n","(Time 14.96 sec; Iteration 2641 / 4000) loss: 1.522015\n","(Time 15.01 sec; Iteration 2651 / 4000) loss: 1.474020\n","(Time 15.06 sec; Iteration 2661 / 4000) loss: 1.439386\n","(Time 15.11 sec; Iteration 2671 / 4000) loss: 1.443060\n","(Time 15.16 sec; Iteration 2681 / 4000) loss: 1.181620\n","(Time 15.21 sec; Iteration 2691 / 4000) loss: 1.268568\n","(Time 15.26 sec; Iteration 2701 / 4000) loss: 1.278015\n","(Time 15.31 sec; Iteration 2711 / 4000) loss: 1.255225\n","(Time 15.36 sec; Iteration 2721 / 4000) loss: 1.162669\n","(Time 15.41 sec; Iteration 2731 / 4000) loss: 1.227514\n","(Time 15.46 sec; Iteration 2741 / 4000) loss: 1.356799\n","(Time 15.51 sec; Iteration 2751 / 4000) loss: 1.235003\n","(Time 15.56 sec; Iteration 2761 / 4000) loss: 1.399809\n","(Time 15.61 sec; Iteration 2771 / 4000) loss: 1.254974\n","(Time 15.66 sec; Iteration 2781 / 4000) loss: 1.417478\n","(Time 15.71 sec; Iteration 2791 / 4000) loss: 1.201602\n","(Epoch 7 / 10) train acc: 0.539000; val_acc: 0.489900\n","(Time 15.93 sec; Iteration 2801 / 4000) loss: 1.325908\n","(Time 15.98 sec; Iteration 2811 / 4000) loss: 1.259086\n","(Time 16.03 sec; Iteration 2821 / 4000) loss: 1.363871\n","(Time 16.08 sec; Iteration 2831 / 4000) loss: 1.341692\n","(Time 16.13 sec; Iteration 2841 / 4000) loss: 1.180890\n","(Time 16.18 sec; Iteration 2851 / 4000) loss: 1.124726\n","(Time 16.23 sec; Iteration 2861 / 4000) loss: 1.398927\n","(Time 16.28 sec; Iteration 2871 / 4000) loss: 1.309478\n","(Time 16.34 sec; Iteration 2881 / 4000) loss: 1.401368\n","(Time 16.39 sec; Iteration 2891 / 4000) loss: 1.377041\n","(Time 16.44 sec; Iteration 2901 / 4000) loss: 1.318557\n","(Time 16.49 sec; Iteration 2911 / 4000) loss: 1.292226\n","(Time 16.54 sec; Iteration 2921 / 4000) loss: 1.508426\n","(Time 16.59 sec; Iteration 2931 / 4000) loss: 1.409223\n","(Time 16.64 sec; Iteration 2941 / 4000) loss: 1.149919\n","(Time 16.69 sec; Iteration 2951 / 4000) loss: 1.287884\n","(Time 16.74 sec; Iteration 2961 / 4000) loss: 1.353141\n","(Time 16.79 sec; Iteration 2971 / 4000) loss: 1.368842\n","(Time 16.84 sec; Iteration 2981 / 4000) loss: 1.188245\n","(Time 16.90 sec; Iteration 2991 / 4000) loss: 1.343926\n","(Time 16.95 sec; Iteration 3001 / 4000) loss: 1.470120\n","(Time 17.00 sec; Iteration 3011 / 4000) loss: 1.311723\n","(Time 17.05 sec; Iteration 3021 / 4000) loss: 1.390933\n","(Time 17.10 sec; Iteration 3031 / 4000) loss: 1.181671\n","(Time 17.15 sec; Iteration 3041 / 4000) loss: 1.360569\n","(Time 17.20 sec; Iteration 3051 / 4000) loss: 1.254770\n","(Time 17.25 sec; Iteration 3061 / 4000) loss: 1.226239\n","(Time 17.30 sec; Iteration 3071 / 4000) loss: 1.171366\n","(Time 17.35 sec; Iteration 3081 / 4000) loss: 1.206862\n","(Time 17.40 sec; Iteration 3091 / 4000) loss: 1.231305\n","(Time 17.45 sec; Iteration 3101 / 4000) loss: 1.246286\n","(Time 17.51 sec; Iteration 3111 / 4000) loss: 1.442142\n","(Time 17.56 sec; Iteration 3121 / 4000) loss: 1.204228\n","(Time 17.61 sec; Iteration 3131 / 4000) loss: 1.289332\n","(Time 17.66 sec; Iteration 3141 / 4000) loss: 1.269922\n","(Time 17.70 sec; Iteration 3151 / 4000) loss: 1.245389\n","(Time 17.75 sec; Iteration 3161 / 4000) loss: 1.221220\n","(Time 17.81 sec; Iteration 3171 / 4000) loss: 1.161363\n","(Time 17.86 sec; Iteration 3181 / 4000) loss: 1.381036\n","(Time 17.91 sec; Iteration 3191 / 4000) loss: 1.286729\n","(Epoch 8 / 10) train acc: 0.529000; val_acc: 0.496400\n","(Time 18.12 sec; Iteration 3201 / 4000) loss: 1.180377\n","(Time 18.17 sec; Iteration 3211 / 4000) loss: 1.409708\n","(Time 18.22 sec; Iteration 3221 / 4000) loss: 1.242363\n","(Time 18.27 sec; Iteration 3231 / 4000) loss: 1.109249\n","(Time 18.32 sec; Iteration 3241 / 4000) loss: 1.294702\n","(Time 18.37 sec; Iteration 3251 / 4000) loss: 1.317802\n","(Time 18.42 sec; Iteration 3261 / 4000) loss: 1.347546\n","(Time 18.47 sec; Iteration 3271 / 4000) loss: 1.235995\n","(Time 18.52 sec; Iteration 3281 / 4000) loss: 1.380530\n","(Time 18.57 sec; Iteration 3291 / 4000) loss: 1.381805\n","(Time 18.62 sec; Iteration 3301 / 4000) loss: 1.360922\n","(Time 18.67 sec; Iteration 3311 / 4000) loss: 1.268379\n","(Time 18.72 sec; Iteration 3321 / 4000) loss: 1.218190\n","(Time 18.77 sec; Iteration 3331 / 4000) loss: 1.279020\n","(Time 18.82 sec; Iteration 3341 / 4000) loss: 1.219954\n","(Time 18.87 sec; Iteration 3351 / 4000) loss: 1.212037\n","(Time 18.93 sec; Iteration 3361 / 4000) loss: 1.360056\n","(Time 18.98 sec; Iteration 3371 / 4000) loss: 1.275914\n","(Time 19.03 sec; Iteration 3381 / 4000) loss: 1.101094\n","(Time 19.08 sec; Iteration 3391 / 4000) loss: 1.436274\n","(Time 19.13 sec; Iteration 3401 / 4000) loss: 1.390154\n","(Time 19.18 sec; Iteration 3411 / 4000) loss: 1.142683\n","(Time 19.23 sec; Iteration 3421 / 4000) loss: 1.312863\n","(Time 19.28 sec; Iteration 3431 / 4000) loss: 1.263322\n","(Time 19.33 sec; Iteration 3441 / 4000) loss: 1.453143\n","(Time 19.38 sec; Iteration 3451 / 4000) loss: 1.384671\n","(Time 19.43 sec; Iteration 3461 / 4000) loss: 1.509826\n","(Time 19.48 sec; Iteration 3471 / 4000) loss: 1.198599\n","(Time 19.53 sec; Iteration 3481 / 4000) loss: 1.238305\n","(Time 19.58 sec; Iteration 3491 / 4000) loss: 1.242721\n","(Time 19.63 sec; Iteration 3501 / 4000) loss: 1.126090\n","(Time 19.68 sec; Iteration 3511 / 4000) loss: 1.109522\n","(Time 19.73 sec; Iteration 3521 / 4000) loss: 1.393798\n","(Time 19.78 sec; Iteration 3531 / 4000) loss: 1.260682\n","(Time 19.83 sec; Iteration 3541 / 4000) loss: 1.296079\n","(Time 19.88 sec; Iteration 3551 / 4000) loss: 1.252848\n","(Time 19.93 sec; Iteration 3561 / 4000) loss: 1.365798\n","(Time 19.99 sec; Iteration 3571 / 4000) loss: 1.337792\n","(Time 20.03 sec; Iteration 3581 / 4000) loss: 1.396553\n","(Time 20.08 sec; Iteration 3591 / 4000) loss: 1.414233\n","(Epoch 9 / 10) train acc: 0.568000; val_acc: 0.506900\n","(Time 20.30 sec; Iteration 3601 / 4000) loss: 1.471752\n","(Time 20.35 sec; Iteration 3611 / 4000) loss: 1.306402\n","(Time 20.39 sec; Iteration 3621 / 4000) loss: 1.383040\n","(Time 20.44 sec; Iteration 3631 / 4000) loss: 1.384934\n","(Time 20.50 sec; Iteration 3641 / 4000) loss: 1.227346\n","(Time 20.55 sec; Iteration 3651 / 4000) loss: 1.154861\n","(Time 20.60 sec; Iteration 3661 / 4000) loss: 1.203838\n","(Time 20.65 sec; Iteration 3671 / 4000) loss: 1.268199\n","(Time 20.69 sec; Iteration 3681 / 4000) loss: 1.264610\n","(Time 20.75 sec; Iteration 3691 / 4000) loss: 1.090994\n","(Time 20.80 sec; Iteration 3701 / 4000) loss: 1.214736\n","(Time 20.85 sec; Iteration 3711 / 4000) loss: 1.398486\n","(Time 20.90 sec; Iteration 3721 / 4000) loss: 1.376327\n","(Time 20.95 sec; Iteration 3731 / 4000) loss: 1.272982\n","(Time 21.00 sec; Iteration 3741 / 4000) loss: 1.278452\n","(Time 21.05 sec; Iteration 3751 / 4000) loss: 1.165819\n","(Time 21.10 sec; Iteration 3761 / 4000) loss: 1.259966\n","(Time 21.15 sec; Iteration 3771 / 4000) loss: 1.371037\n","(Time 21.20 sec; Iteration 3781 / 4000) loss: 1.139055\n","(Time 21.25 sec; Iteration 3791 / 4000) loss: 1.253244\n","(Time 21.30 sec; Iteration 3801 / 4000) loss: 1.424030\n","(Time 21.35 sec; Iteration 3811 / 4000) loss: 1.281586\n","(Time 21.40 sec; Iteration 3821 / 4000) loss: 1.373167\n","(Time 21.45 sec; Iteration 3831 / 4000) loss: 1.254153\n","(Time 21.50 sec; Iteration 3841 / 4000) loss: 1.246466\n","(Time 21.55 sec; Iteration 3851 / 4000) loss: 1.205352\n","(Time 21.60 sec; Iteration 3861 / 4000) loss: 1.563120\n","(Time 21.65 sec; Iteration 3871 / 4000) loss: 1.237035\n","(Time 21.70 sec; Iteration 3881 / 4000) loss: 1.333726\n","(Time 21.75 sec; Iteration 3891 / 4000) loss: 1.190968\n","(Time 21.80 sec; Iteration 3901 / 4000) loss: 1.442500\n","(Time 21.85 sec; Iteration 3911 / 4000) loss: 1.306660\n","(Time 21.90 sec; Iteration 3921 / 4000) loss: 1.037206\n","(Time 21.95 sec; Iteration 3931 / 4000) loss: 1.385250\n","(Time 22.00 sec; Iteration 3941 / 4000) loss: 1.100189\n","(Time 22.05 sec; Iteration 3951 / 4000) loss: 1.295998\n","(Time 22.10 sec; Iteration 3961 / 4000) loss: 1.167010\n","(Time 22.15 sec; Iteration 3971 / 4000) loss: 1.510381\n","(Time 22.20 sec; Iteration 3981 / 4000) loss: 1.384203\n","(Time 22.25 sec; Iteration 3991 / 4000) loss: 1.292444\n","(Epoch 10 / 10) train acc: 0.581000; val_acc: 0.506100\n","load checkpoint file: /content/drive/MyDrive/Mich.ComputerVisionAssignment1/A3/best_two_layer_net.pth\n","Saved model's accuracy on validation is 0.5230000019073486\n"]}],"source":["path = os.path.join(GOOGLE_DRIVE_PATH, 'best_two_layer_net.pth')\n","solver.model.save(path)\n","\n","# Create a new instance\n","from fully_connected_networks import create_solver_instance\n","reset_seed(0)\n","\n","solver = create_solver_instance(data_dict=data_dict, dtype=torch.float64, device='cuda')\n","\n","# Load model\n","solver.model.load(path, dtype=torch.float64, device='cuda')\n","\n","# Evaluate on validation set\n","accuracy = solver.check_accuracy(solver.X_val, solver.y_val)\n","print(f\"Saved model's accuracy on validation is {accuracy}\")\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"eNyFLT1We1DI","new_sheet":false,"run_control":{"read_only":false}},"source":["# Multilayer network\n","Next you will implement a fully-connected network with an arbitrary number of hidden layers.\n","\n","Read through the `FullyConnectedNet` class in `fully_connected_networks.py`. Implement the initialization, the forward pass, and the backward pass. For the moment don't worry about implementing dropout; we will add this feature soon."]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"3abR1_qhe1DK","new_sheet":false,"run_control":{"read_only":false}},"source":["## Initial loss and gradient check\n","\n","As a sanity check, run the following to check the initial loss and to gradient check the network both with and without regularization. Do the initial losses seem reasonable?\n","\n","For gradient checking, you should expect to see errors less than `1e-6`, except for the check on `W1` and `W2` with `reg=0` where your errors should be less than `1e-5`."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"1waPtKRDe1DL","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import FullyConnectedNet\n","\n","reset_seed(0)\n","N, D, H1, H2, C = 2, 15, 20, 30, 10\n","X = torch.randn(N, D, dtype=torch.float64, device='cuda')\n","y = torch.randint(C, size=(N,), dtype=torch.int64, device='cuda')\n","\n","for reg in [0, 3.14]:\n","  print('Running check with reg = ', reg)\n","  model = FullyConnectedNet(\n","        [H1, H2], \n","        input_dim=D,\n","        num_classes=C,\n","        reg=reg,\n","        weight_scale=5e-2, \n","        dtype=torch.float64, \n","        device='cuda'\n","  )\n","\n","  loss, grads = model.loss(X, y)\n","  print('Initial loss: ', loss.item())\n","\n","  for name in sorted(grads):\n","    f = lambda _: model.loss(X, y)[0]\n","    grad_num = eecs598.grad.compute_numeric_gradient(f, model.params[name])\n","    print('%s relative error: %.2e' % (name, eecs598.grad.rel_error(grad_num, grads[name])))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"-q6aWzNfe1DQ","new_sheet":false,"run_control":{"read_only":false}},"source":["As another sanity check, make sure you can overfit a small dataset of 50 images. First we will try a three-layer network with 100 units in each hidden layer. In the following cell, tweak the **learning rate** and **weight initialization scale** to overfit and achieve 100% training accuracy within 20 epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"2NccCDJ3e1DR","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import FullyConnectedNet, get_three_layer_network_params\n","\n","# TODO: Use a three-layer Net to overfit 50 training examples by \n","# tweaking just the learning rate and initialization scale.\n","reset_seed(0)\n","\n","num_train = 50\n","small_data = {\n","  'X_train': data_dict['X_train'][:num_train],\n","  'y_train': data_dict['y_train'][:num_train],\n","  'X_val': data_dict['X_val'],\n","  'y_val': data_dict['y_val'],\n","}\n","\n","# Update parameters in get_three_layer_network_params\n","weight_scale, learning_rate = get_three_layer_network_params()\n","\n","model = FullyConnectedNet([100, 100],\n","              weight_scale=weight_scale, dtype=torch.float32, device='cuda')\n","solver = Solver(model, small_data,\n","                print_every=10, num_epochs=20, batch_size=25,\n","                optim_config={\n","                  'learning_rate': learning_rate,\n","                },\n","                device='cuda',\n","         )\n","solver.train()\n","\n","plt.plot(solver.loss_history, 'o')\n","plt.title('Training loss history')\n","plt.xlabel('Iteration')\n","plt.ylabel('Training loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"tskjw8VUe1DV","new_sheet":false,"run_control":{"read_only":false}},"source":["Now try to use a five-layer network with 100 units on each layer to overfit 50 training examples. Again, you will have to adjust the learning rate and weight initialization scale, but you should be able to achieve 100% training accuracy within 20 epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"D5mAWrrPe1Dc","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import FullyConnectedNet, get_five_layer_network_params\n","\n","# TODO: Use a five-layer Net to overfit 50 training examples by \n","# tweaking just the learning rate and initialization scale.\n","reset_seed(0)\n","\n","num_train = 50\n","small_data = {\n","  'X_train': data_dict['X_train'][:num_train],\n","  'y_train': data_dict['y_train'][:num_train],\n","  'X_val': data_dict['X_val'],\n","  'y_val': data_dict['y_val'],\n","}\n","\n","\n","# Update parameters in get_three_layer_network_params\n","weight_scale, learning_rate = get_five_layer_network_params()\n","\n","# Run models and solver with parameters\n","model = FullyConnectedNet([100, 100, 100, 100],\n","                weight_scale=weight_scale, dtype=torch.float32, device='cuda')\n","solver = Solver(model, small_data,\n","                print_every=10, num_epochs=20, batch_size=25,\n","                optim_config={\n","                  'learning_rate': learning_rate,\n","                },\n","                device='cuda',\n","         )\n","# Turn off keep_best_params to allow final weights to be saved, instead of best weights on validation set.\n","solver.train(return_best_params=False)\n","\n","plt.plot(solver.loss_history, 'o')\n","plt.title('Training loss history')\n","plt.xlabel('Iteration')\n","plt.ylabel('Training loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"3M2JQjj_93RW","new_sheet":false,"run_control":{"read_only":false}},"source":["If you're satisfied with your model's performance, save the overfit model. Just a sanity check, we evaluate it one the training set again to verify that the saved weights have the correct performance. "]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"YAnM8V9z938Q","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["# Set path \n","path = os.path.join(GOOGLE_DRIVE_PATH, 'best_overfit_five_layer_net.pth')\n","solver.model.save(path)\n","\n","\n","# Create a new instance  -- Note that hidden dims being different doesn't matter here.\n","model = FullyConnectedNet(hidden_dims=[100, ], dtype=torch.float32, device='cuda')\n","solver = Solver(model, small_data,\n","                print_every=10, num_epochs=20, batch_size=25,\n","                optim_config={\n","                  'learning_rate': learning_rate,\n","                },\n","                device='cuda',\n","         )\n","\n","\n","# Load model\n","solver.model.load(path, dtype=torch.float32, device='cuda')\n","\n","# Evaluate on validation set\n","accuracy = solver.check_accuracy(solver.X_train, solver.y_train)\n","print(f\"Saved model's accuracy on small train is {accuracy}\")\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"T4eWrnY7e1Di","new_sheet":false,"run_control":{"read_only":false}},"source":["# Update rules\n","So far we have used vanilla stochastic gradient descent (SGD) as our update rule. More sophisticated update rules can make it easier to train deep networks. We will implement a few of the most commonly used update rules and compare them to vanilla SGD."]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"zBDJqbeVe1Dn","new_sheet":false,"run_control":{"read_only":false}},"source":["## SGD+Momentum\n","Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochastic gradient descent. See the Momentum Update section at http://cs231n.github.io/neural-networks-3/#sgd for more information.\n","\n","We will implement various first-order update rules that are commonly used\n","for training neural networks. Each update rule accepts current weights and the\n","gradient of the loss with respect to those weights and produces the next set of\n","weights. Each update rule has the same interface:\n","```python\n","def update(w, dw, config=None):\n","Inputs:\n","  - w: A tensor giving the current weights.\n","  - dw: A tensor of the same shape as w giving the gradient of the\n","    loss with respect to w.\n","  - config: A dictionary containing hyperparameter values such as learning\n","    rate, momentum, etc. If the update rule requires caching values over many\n","    iterations, then config will also hold these cached values.\n","Returns:\n","  - next_w: The next point after the update.\n","  - config: The config dictionary to be passed to the next iteration of the\n","    update rule.\n","NOTE: For most update rules, the default learning rate will probably not\n","perform well; however the default values of the other hyperparameters should\n","work well for a variety of different problems.\n","For efficiency, update rules may perform in-place updates, mutating w and\n","setting next_w equal to w.\n","```\n","We provide the implementation of the SGD update rule for your reference in `fully_connected_networks.py`\n","\n","Now **implement** the SGD+Momentum update rule using the same interface. Run the following to check your implementation of SGD+Momentum. You should see errors less than `1e-7`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"RbQrkNo_e1Dp","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import sgd_momentum\n","\n","reset_seed(0)\n","\n","N, D = 4, 5\n","w = torch.linspace(-0.4, 0.6, steps=N*D, dtype=torch.float64, device='cuda').reshape(N, D)\n","dw = torch.linspace(-0.6, 0.4, steps=N*D, dtype=torch.float64, device='cuda').reshape(N, D)\n","v = torch.linspace(0.6, 0.9, steps=N*D, dtype=torch.float64, device='cuda').reshape(N, D)\n","\n","config = {'learning_rate': 1e-3, 'velocity': v}\n","next_w, _ = sgd_momentum(w, dw, config=config)\n","\n","expected_next_w = torch.tensor([\n","  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n","  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n","  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n","  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]],\n","   dtype=torch.float64, device='cuda')\n","expected_velocity = torch.tensor([\n","  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n","  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n","  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n","  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]],\n","   dtype=torch.float64, device='cuda')\n","\n","# Should see relative errors around e-8 or less\n","print('next_w error: ', eecs598.grad.rel_error(next_w, expected_next_w))\n","print('velocity error: ', eecs598.grad.rel_error(expected_velocity, config['velocity']))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"7QQj73zje1D2","new_sheet":false,"run_control":{"read_only":false}},"source":["Once you have done so, run the following to train a six-layer network with both SGD and SGD+momentum. You should see the SGD+momentum update rule converge faster."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"qXdMNC9Ve1D4","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import FullyConnectedNet, sgd, sgd_momentum\n","\n","# TODO: Use a three-layer Net to overfit 50 training examples by \n","# tweaking just the learning rate and initialization scale.\n","reset_seed(0)\n","\n","num_train = 4000\n","small_data = {\n","  'X_train': data_dict['X_train'][:num_train],\n","  'y_train': data_dict['y_train'][:num_train],\n","  'X_val': data_dict['X_val'],\n","  'y_val': data_dict['y_val'],\n","}\n","\n","solvers = {}\n","\n","for update_rule_name, update_rule_fn in [('sgd', sgd), ('sgd_momentum', sgd_momentum)]:\n","  print('running with ', update_rule_name)\n","  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2,\n","                            dtype=torch.float32, device='cuda')\n","\n","  solver = Solver(model, small_data,\n","                  num_epochs=5, batch_size=100,\n","                  update_rule=update_rule_fn,\n","                  optim_config={\n","                    'learning_rate': 5e-2,\n","                  },\n","                  print_every=1000,\n","                  verbose=True,\n","                  device='cuda')\n","  solvers[update_rule_name] = solver\n","  solver.train()\n","  print()\n","  \n","plt.subplot(3, 1, 1)\n","plt.title('Training loss')\n","plt.xlabel('Iteration')\n","for update_rule, solver in solvers.items():\n","  plt.plot(solver.loss_history, 'o', label=\"loss_%s\" % update_rule)\n","plt.legend(loc='lower center', ncol=4)\n","  \n","plt.subplot(3, 1, 2)\n","plt.title('Training accuracy')\n","plt.xlabel('Epoch')\n","for update_rule, solver in solvers.items():\n","  plt.plot(solver.train_acc_history, '-o', label=\"train_acc_%s\" % update_rule)\n","plt.legend(loc='lower center', ncol=4)\n","\n","  \n","plt.subplot(3, 1, 3)\n","plt.title('Validation accuracy')\n","plt.xlabel('Epoch')\n","for update_rule, solver in solvers.items():\n","  plt.plot(solver.val_acc_history, '-o', label=\"val_acc_%s\" % update_rule)\n","plt.legend(loc='lower center', ncol=4)\n","\n","plt.gcf().set_size_inches(10, 20)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"wYtKqDdEe1D-","new_sheet":false,"run_control":{"read_only":false}},"source":["## RMSProp\n","RMSProp [1] is an update rule that set per-parameter learning rates by using a running average of the second moments of gradients.\n","\n","**Implement** the RMSProp update rule in the `rmsprop` function in `fully_connected_networks.py`. Run the following to test your RMSProp implementation. You should see errors less than `1e-6`.\n","\n","[1] Tijmen Tieleman and Geoffrey Hinton. \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.\" COURSERA: Neural Networks for Machine Learning 4 (2012)."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"RBBpJhJie1D_","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["# Test RMSProp implementation\n","from fully_connected_networks import rmsprop\n","\n","reset_seed(0)\n","\n","N, D = 4, 5\n","w = torch.linspace(-0.4, 0.6, steps=N*D, dtype=torch.float64, device='cuda').reshape(N, D)\n","dw = torch.linspace(-0.6, 0.4, steps=N*D, dtype=torch.float64, device='cuda').reshape(N, D)\n","cache = torch.linspace(0.6, 0.9, steps=N*D, dtype=torch.float64, device='cuda').reshape(N, D)\n","\n","config = {'learning_rate': 1e-2, 'cache': cache}\n","next_w, _ = rmsprop(w, dw, config=config)\n","\n","expected_next_w = torch.tensor([\n","  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n","  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n","  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n","  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]],\n","   dtype=torch.float64, device='cuda')\n","expected_cache = torch.tensor([\n","  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n","  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n","  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n","  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]],\n","   dtype=torch.float64, device='cuda')\n","\n","print('next_w error: ', eecs598.grad.rel_error(expected_next_w, next_w))\n","print('cache error: ', eecs598.grad.rel_error(expected_cache, config['cache']))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"bMdq7WRFDiJw","new_sheet":false,"run_control":{"read_only":false}},"source":["## Adam\n","Adam [2] extends RMSprop with a first-order gradient cache similar to momentum, and a bias correction mechanism to prevent large steps at the start of optimization. Adam is one of the most commonly used update rules used in practice for training deep neural networks.\n","\n","Implement the Adam update rule in the `adam` function in `fully_connected_networks.py`. Run the following to test your Adam implementation. You should see error less than `1e-6` for `next_w`, and errors less than `1e-8` for `v` and `m`:\n","\n","\n","\n","**NOTE:** Please implement the _complete_ Adam update rule (with the bias correction mechanism), not the first simplified version mentioned in the course notes. \n","\n","[2] Diederik Kingma and Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", ICLR 2015."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"ovUXV51Le1EE","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["# Test Adam implementation\n","from fully_connected_networks import adam\n","\n","reset_seed(0)\n","\n","N, D = 4, 5\n","w = torch.linspace(-0.4, 0.6, steps=N*D, dtype=torch.float64, device='cuda').reshape(N, D)\n","dw = torch.linspace(-0.6, 0.4, steps=N*D, dtype=torch.float64, device='cuda').reshape(N, D)\n","m = torch.linspace(0.6, 0.9, steps=N*D, dtype=torch.float64, device='cuda').reshape(N, D)\n","v = torch.linspace(0.7, 0.5, steps=N*D, dtype=torch.float64, device='cuda').reshape(N, D)\n","\n","config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n","next_w, _ = adam(w, dw, config=config)\n","\n","expected_next_w = torch.tensor([\n","  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n","  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n","  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n","  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]],\n","   dtype=torch.float64, device='cuda')\n","expected_v = torch.tensor([\n","  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n","  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n","  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n","  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]],\n","   dtype=torch.float64, device='cuda')\n","expected_m = torch.tensor([\n","  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n","  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n","  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n","  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]],\n","   dtype=torch.float64, device='cuda')\n","\n","# You should see relative errors around e-7 or less\n","print('next_w error: ', eecs598.grad.rel_error(expected_next_w, next_w))\n","print('v error: ', eecs598.grad.rel_error(expected_v, config['v']))\n","print('m error: ', eecs598.grad.rel_error(expected_m, config['m']))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"1T_qzgxte1EI","new_sheet":false,"run_control":{"read_only":false}},"source":["Once you have debugged your RMSProp and Adam implementations, run the following to train a pair of deep networks using these new update rules:"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"6TFopQgre1EJ","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["# Test Adam implementation\n","from fully_connected_networks import adam, rmsprop, FullyConnectedNet\n","\n","for update_rule_name, update_rule_fn, learning_rate in [('adam', adam, 1e-3), ('rmsprop', rmsprop, 1e-4)]:\n","  print('running with ', update_rule)\n","  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2, device='cuda')\n","\n","  solver = Solver(model, small_data,\n","                  num_epochs=5, batch_size=100,\n","                  update_rule=update_rule_fn,\n","                  optim_config={\n","                    'learning_rate': learning_rate\n","                  },\n","                  print_every=1000,\n","                  verbose=True, device='cuda')\n","  solvers[update_rule_name] = solver\n","  solver.train()\n","  print()\n","\n","plt.subplot(3, 1, 1)\n","plt.title('Training loss')\n","plt.xlabel('Iteration')\n","for update_rule, solver in list(solvers.items()):\n","  plt.plot(solver.loss_history, 'o', label=update_rule)\n","plt.legend(loc='lower center', ncol=4)\n","  \n","plt.subplot(3, 1, 2)\n","plt.title('Training accuracy')\n","plt.xlabel('Epoch')\n","for update_rule, solver in list(solvers.items()):\n","  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n","plt.legend(loc='lower center', ncol=4)\n","\n","plt.subplot(3, 1, 3)\n","plt.title('Validation accuracy')\n","plt.xlabel('Epoch')\n","for update_rule, solver in list(solvers.items()):\n","  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n","plt.legend(loc='lower center', ncol=4)\n","\n","plt.gcf().set_size_inches(10, 20)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"C2_BL-2TwxKR","new_sheet":false,"run_control":{"read_only":false},"tags":["pdf-title"]},"source":["# Dropout\n","Dropout [1] is a technique for regularizing neural networks by randomly setting some output activations to zero during the forward pass. In this exercise you will implement a dropout layer and modify your fully-connected network to optionally use dropout.\n","\n","[1] [Geoffrey E. Hinton et al, \"Improving neural networks by preventing co-adaptation of feature detectors\", arXiv 2012](https://arxiv.org/abs/1207.0580)"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"s68cb0QBwxKj","new_sheet":false,"run_control":{"read_only":false}},"source":["## Dropout: forward\n","**Implement** the forward pass for dropout in `fully_connected_networks.py`. Since dropout behaves differently during training and testing, make sure to implement the operation for both modes.\n","\n","Run the following to test your dropout implementation. The mean of the output should be approximately the same during training and testing. During training the number of outputs set to zero should be approximately equal to the drop probability `p`, and during testing no outputs should be set to zero."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"vFAmI9VxwxKk","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import Dropout\n","\n","reset_seed(0)\n","x = torch.randn(500, 500, dtype=torch.float64, device='cuda') + 10\n","\n","for p in [0.25, 0.4, 0.7]:\n","  out, _ = Dropout.forward(x, {'mode': 'train', 'p': p})\n","  out_test, _ = Dropout.forward(x, {'mode': 'test', 'p': p})\n","\n","  print('Running tests with p = ', p)\n","  print('Mean of input: ', x.mean().item())\n","  print('Mean of train-time output: ', out.mean().item())\n","  print('Mean of test-time output: ', out_test.mean().item())\n","  print('Fraction of train-time output set to zero: ', (out == 0).type(torch.float32).mean().item())\n","  print('Fraction of test-time output set to zero: ', (out_test == 0).type(torch.float32).mean().item())\n","  print()"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"dt2BpwxswxKn","new_sheet":false,"run_control":{"read_only":false}},"source":["## Dropout: backward\n","Implement the backward pass for dropout. After doing so, run the following cell to numerically gradient-check your implementation."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"3uctLwyIwxKo","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import Dropout\n","\n","reset_seed(0)\n","x = torch.randn(10, 10, dtype=torch.float64, device='cuda') + 10\n","dout = torch.randn_like(x)\n","\n","dropout_param = {'mode': 'train', 'p': 0.2, 'seed': 0}\n","out, cache = Dropout.forward(x, dropout_param)\n","dx = Dropout.backward(dout, cache)\n","dx_num = eecs598.grad.compute_numeric_gradient(lambda xx: Dropout.forward(xx, dropout_param)[0], x, dout)\n","\n","# Error should be around e-10 or less\n","print('dx relative error: ', eecs598.grad.rel_error(dx, dx_num))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"OLzMLx-iwxKs","new_sheet":false,"run_control":{"read_only":false}},"source":["# Fully-connected nets with dropout\n","Modify [your implementation](#scrollTo=7p-goSyucyZH) of `FullyConnectedNet` to use dropout. Specifically, if the constructor of the network receives a value that is not 0 for the `dropout` parameter, then the net should add a dropout layer immediately after every ReLU nonlinearity.\n","\n","After doing so, run the following to numerically gradient-check your implementation. You should see errors less than `1e-5`, and different dropout rates should result different error values."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"18ugsX0iwxKu","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import FullyConnectedNet\n","\n","reset_seed(0)\n","\n","N, D, H1, H2, C = 2, 15, 20, 30, 10\n","X = torch.randn(N, D, dtype=torch.float64, device='cuda')\n","y = torch.randint(C, size=(N,), dtype=torch.int64, device='cuda')\n","\n","for dropout in [0, 0.25, 0.5]:\n","  print('Running check with dropout = ', dropout)\n","  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n","                            weight_scale=5e-2, dropout=dropout,\n","                            seed=0, dtype=torch.float64, device='cuda')\n","\n","  loss, grads = model.loss(X, y)\n","  print('Initial loss: ', loss.item())\n","  \n","  # Relative errors should be around e-5 or less.\n","  for name in sorted(grads):\n","    f = lambda _: model.loss(X, y)[0]\n","    grad_num = eecs598.grad.compute_numeric_gradient(f, model.params[name])\n","    print('%s relative error: %.2e' % (name, eecs598.grad.rel_error(grad_num, grads[name])))\n","  print()"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"dmhrgg5hwxKy","new_sheet":false,"run_control":{"read_only":false}},"source":["## Regularization experiment\n","To get a sense of the way that dropout can regularize a neural network, we will train three different two-layer networks:\n","\n","1. Hidden size 256, dropout = 0\n","2. Hidden size 512, dropout = 0\n","3. Hidden size 512, dropout = 0.5\n","\n","We will then visualize the training and validation accuracies of these three networks."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"N6srh4BLwxKz","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import FullyConnectedNet\n","\n","# Train two identical nets, one with dropout and one without\n","reset_seed(0)\n","num_train = 20000\n","small_data = {\n","  'X_train': data_dict['X_train'][:num_train],\n","  'y_train': data_dict['y_train'][:num_train],\n","  'X_val': data_dict['X_val'],\n","  'y_val': data_dict['y_val'],\n","}\n","\n","solvers = {}\n","dropout_choices = [0, 0, 0.5]\n","width_choices = [256, 512, 512]\n","for dropout, width in zip(dropout_choices, width_choices):\n","# for dropout in dropout_choices:\n","  model = FullyConnectedNet([width], dropout=dropout, dtype=torch.float32, device='cuda')\n","  print('Training a model with dropout=%.2f and width=%d' % (dropout, width))\n","\n","  solver = Solver(model, small_data,\n","                  num_epochs=100, batch_size=512,\n","                  update_rule=adam,\n","                  optim_config={\n","                    'learning_rate': 5e-3,\n","                  },\n","                  print_every=100000, print_acc_every=10,\n","                  verbose=True, device='cuda')\n","  solver.train()\n","  solvers[(dropout, width)] = solver\n","  print()"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"mjdKh3apRtX4","new_sheet":false,"run_control":{"read_only":false}},"source":["If everything worked as expected, you should see that the network with dropout has lower training accuracies than the networks without dropout, but that it achieves higher validation accuracies.\n","\n","You should also see that a network with width 512 and dropout 0.5 achieves higher validation accuracies than a network with width 256 and no dropout. This demonstrates that reducing the model size is not generally an effective regularization strategy -- it's often better to use a larger model with explicit regularization."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"aCDhFCR0wxK2","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["plt.subplot(2, 1, 1)\n","for (dropout, width), solver in solvers.items():\n","  train_acc = solver.train_acc_history\n","  label = 'dropout=%.2f, width=%d' % (dropout, width)\n","  plt.plot(train_acc, 'o', label=label)\n","plt.title('Train accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend(ncol=2, loc='lower right')\n","  \n","plt.subplot(2, 1, 2)\n","for (dropout, width), solver in solvers.items():\n","  val_acc = solver.val_acc_history\n","  label = 'dropout=%.2f, width=%d' % (dropout, width)\n","  plt.plot(val_acc, 'o', label=label)\n","plt.ylim(0.4, 0.52)\n","plt.title('Val accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend(ncol=2, loc='lower right')\n","\n","plt.gcf().set_size_inches(10, 15)\n","plt.show()"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["FrfeHl_-m4V-"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.0"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"39dfa3c6eb594d50b5caae902427e54e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c849dbdfdd94403cb12436580b03a77e","IPY_MODEL_0b06e1193c094791bb4da0d537ff4aee","IPY_MODEL_0748226e6f0e47438585d8db3d66490e"],"layout":"IPY_MODEL_c75907f5539f4740b38ffe5105af2bd4"}},"c849dbdfdd94403cb12436580b03a77e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_819fdc5a53b6486e9bb5f539faaca829","placeholder":"​","style":"IPY_MODEL_703f2de518124ba99b19f44299fa5664","value":"100%"}},"0b06e1193c094791bb4da0d537ff4aee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7764548275034dfbaf3092ff092ce15d","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bd62f274ddf34fb8a95b42b10ec62394","value":170498071}},"0748226e6f0e47438585d8db3d66490e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc9b50a9957141538b417210eb32ff54","placeholder":"​","style":"IPY_MODEL_edc0d90cf2e3406390ec7122d15b08cf","value":" 170498071/170498071 [00:05&lt;00:00, 33012788.41it/s]"}},"c75907f5539f4740b38ffe5105af2bd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"819fdc5a53b6486e9bb5f539faaca829":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"703f2de518124ba99b19f44299fa5664":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7764548275034dfbaf3092ff092ce15d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd62f274ddf34fb8a95b42b10ec62394":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fc9b50a9957141538b417210eb32ff54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"edc0d90cf2e3406390ec7122d15b08cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}